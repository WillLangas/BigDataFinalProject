{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Reddit Comment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"p1dG7hgoowK4BSlUdar1WQ\",\n",
    "    client_secret=\"pEePtSnw7KMDZi6fCzkKaOth_pgKpQ\",\n",
    "    password=\"outdoortuesday\",\n",
    "    user_agent=\"Big Data by u/DISWillJayminMaya \",\n",
    "    username=\"DISWillJayminMaya \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def returnListofScores(listofcomments):\n",
    "    templist=[]\n",
    "    for com in listofcomments:\n",
    "        temp = analyzer.polarity_scores(com)\n",
    "        compScore = temp['compound']\n",
    "        templist.append(compScore)\n",
    "    return templist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "subreddit = reddit.subreddit('wallstreetbets')\n",
    "\n",
    "scoreDict={key:list() for key in tickerlist}\n",
    "\n",
    "hot = subreddit.hot(limit=25) # getting first 15 posts in the 'hot' section of the subreddit\n",
    "sum = [0] * len(tickerlist) # our output array\n",
    "counttotal = 0 # total number of comment read\n",
    "submissions_counter = 0\n",
    "\n",
    "rel_comments = [] # List of comments that are relevant to the ticker list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returnCompScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16416/4024818855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[0mrel_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                         \u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturnCompScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                         \u001b[0mscoreDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'returnCompScore' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop for fetching the comments and the amount of time each ticker is mentioned\n",
    "for submissions in hot:\n",
    "    if not submissions.stickied:\n",
    "        submissions_counter+=1\n",
    "        if submissions_counter > 5:\n",
    "            comments = submissions.comments\n",
    "            for comment in comments:\n",
    "                if isinstance(comment, MoreComments):\n",
    "                    continue\n",
    "                counttotal+=1\n",
    "                for i, ticker in enumerate(tickerlist):\n",
    "                    if ticker in comment.body:\n",
    "                        rel_comments.append(comment.body)\n",
    "                        sum[i]=sum[i]+1\n",
    "                        temp = returnCompScore(comment.body)\n",
    "                        scoreDict[ticker].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDict={key:list() for key in tickerlist}\n",
    "spreadDict={key:list() for key in tickerlist}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    if len(scoreDict[tick])>0:\n",
    "        meanDict[tick]=statistics.mean(scoreDict[tick])\n",
    "    if len (scoreDict[tick])>1:\n",
    "        spreadDict[tick]=statistics.stdev(scoreDict[tick])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n",
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n"
     ]
    }
   ],
   "source": [
    "print(meanDict)\n",
    "print(spreadDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments read:  249\n",
      "  Tick  Counts\n",
      "0  GME       1\n"
     ]
    }
   ],
   "source": [
    "output=pd.DataFrame(data={'Tick': tickerlist, 'Counts': sum})\n",
    "print('Total comments read: ',counttotal)\n",
    "print(output[output['Counts']>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Stock Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json, csv\n",
    "\n",
    "size = 'compact' # 'full' for complete historical data, 'compact' for most recent 100\n",
    "ticker = ['GME', 'SPY', 'TWTTR', 'TSLA', 'AMD'] # stock tickers to search for\n",
    "datatype = 'csv' # 'json' for JSON output, 'csv' for CSV output\n",
    "\n",
    "for stock in ticker:\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={stock}&outputsize={size}&datatype={datatype}&apikey=QC1C7LRPUTLC597Q'\n",
    "    response = requests.get(url)\n",
    "    #Save CSV to file\n",
    "    with open(f'{stock}.csv', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>86.670</td>\n",
       "      <td>90.580</td>\n",
       "      <td>84.7800</td>\n",
       "      <td>89.64</td>\n",
       "      <td>91495449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>84.250</td>\n",
       "      <td>87.900</td>\n",
       "      <td>84.0200</td>\n",
       "      <td>84.91</td>\n",
       "      <td>83125054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>89.740</td>\n",
       "      <td>90.120</td>\n",
       "      <td>85.0800</td>\n",
       "      <td>85.16</td>\n",
       "      <td>87805574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>89.860</td>\n",
       "      <td>91.370</td>\n",
       "      <td>88.6100</td>\n",
       "      <td>90.69</td>\n",
       "      <td>93481042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>90.030</td>\n",
       "      <td>91.460</td>\n",
       "      <td>87.9350</td>\n",
       "      <td>88.14</td>\n",
       "      <td>75017652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>141.290</td>\n",
       "      <td>141.365</td>\n",
       "      <td>135.8200</td>\n",
       "      <td>138.55</td>\n",
       "      <td>42224275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>145.160</td>\n",
       "      <td>146.690</td>\n",
       "      <td>137.8000</td>\n",
       "      <td>138.10</td>\n",
       "      <td>53019926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>144.960</td>\n",
       "      <td>147.040</td>\n",
       "      <td>142.7000</td>\n",
       "      <td>145.24</td>\n",
       "      <td>40977478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>143.900</td>\n",
       "      <td>145.760</td>\n",
       "      <td>141.0001</td>\n",
       "      <td>144.85</td>\n",
       "      <td>53359432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>141.135</td>\n",
       "      <td>141.310</td>\n",
       "      <td>134.2000</td>\n",
       "      <td>139.06</td>\n",
       "      <td>66776493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp     open     high       low   close    volume\n",
       "0   2022-04-28   86.670   90.580   84.7800   89.64  91495449\n",
       "1   2022-04-27   84.250   87.900   84.0200   84.91  83125054\n",
       "2   2022-04-26   89.740   90.120   85.0800   85.16  87805574\n",
       "3   2022-04-25   89.860   91.370   88.6100   90.69  93481042\n",
       "4   2022-04-22   90.030   91.460   87.9350   88.14  75017652\n",
       "..         ...      ...      ...       ...     ...       ...\n",
       "95  2021-12-10  141.290  141.365  135.8200  138.55  42224275\n",
       "96  2021-12-09  145.160  146.690  137.8000  138.10  53019926\n",
       "97  2021-12-08  144.960  147.040  142.7000  145.24  40977478\n",
       "98  2021-12-07  143.900  145.760  141.0001  144.85  53359432\n",
       "99  2021-12-06  141.135  141.310  134.2000  139.06  66776493\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amd_data = pd.read_csv(\"AMD.csv\")\n",
    "amd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get avg, spread, median of sentiment of comments in any given period\n",
    "    compare this with the performance of the stock\n",
    "\n",
    "model:\n",
    "    based on last x days of reddit comments, what is the price going to be?\n",
    "    take data from x days, put it all into one vector, and predict from this\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a function to return all comments that mention a stock based on a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def make_request(uri, max_retries = 5):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)\n",
    "\n",
    "def get_intervals(startPOSTIX, endPOSTIX, daysInInterval = 1):\n",
    "    \"\"\"\n",
    "    get_intervals goes day by day through the start and end dates, returning that day's POSTIX\n",
    "    \"\"\"\n",
    "    # 86,400 seconds in a day:\n",
    "    period = (86400 * daysInInterval)\n",
    "    end = startPOSTIX + period\n",
    "    \n",
    "    yield(int(startPOSTIX), int(end))\n",
    "    \n",
    "    padding = 1\n",
    "    while end <= endPOSTIX:\n",
    "        startPOSTIX = end + padding\n",
    "        end = (startPOSTIX - padding) + period\n",
    "        yield int(startPOSTIX), int(end)\n",
    "    \n",
    "    \n",
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts(\n",
    "        make_request(URI_TEMPLATE.format\n",
    "                     (subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request( \\\n",
    "                URI_TEMPLATE.format( \\\n",
    "                    subreddit, new_start_at, end_at, bSIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "    return post_collections\n",
    "\n",
    "def get_comments_by_date (startDate, endDate, subreddit='wallstreetbets'):\n",
    "    \"\"\"\n",
    "    Takes a given time interval and scrapes the given subreddit for all of the comments\n",
    "    that relate to the given ticker name, returning them as an array. Basic structure taken \n",
    "    from medium article.\n",
    "    \"\"\"\n",
    "    # Converting start and end dates to POSTIX:\n",
    "    startDate = math.floor(startDate.timestamp())\n",
    "    endDate = math.floor(endDate.timestamp())\n",
    "\n",
    "    posts = []\n",
    "    # This loop gets all of the posts in the given timeframe\n",
    "    for interval in get_intervals(startDate, endDate):\n",
    "        print(\" -- Fetching Posts From: \", datetime.fromtimestamp(interval[0]), \" to \", datetime.fromtimestamp(interval[1]))\n",
    "        pulled_posts = pull_posts_for(subreddit, interval[0], interval[1])\n",
    "        posts.extend(pulled_posts)\n",
    "        time.sleep(.100) # So as not to over request reddit\n",
    "    \n",
    "    TIMEOUT_SECS = .100\n",
    "    \n",
    "    reddit_posts = []\n",
    "    reddit_comments = {}\n",
    "    reddit_comments[startDate] = []\n",
    "    # Going through each unique post and comment and adding them to the relevant arrays\n",
    "    for sub_id in np.unique([post['id'] for post in posts]):\n",
    "        sub = reddit.submission(id=sub_id)\n",
    "        reddit_posts.append(sub)\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        # Looping through each comment:\n",
    "        for comment in sub.comments.list(): \n",
    "            reddit_comments[startDate].append(comment.body)\n",
    "                \n",
    "            time.sleep(TIMEOUT_SECS)\n",
    "    \n",
    "    return reddit_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Fetching Posts From:  2022-04-29 01:18:32  to  2022-04-30 01:18:32\n"
     ]
    }
   ],
   "source": [
    "start = datetime.today() - timedelta(days=0.5)\n",
    "end = datetime.today()\n",
    "\n",
    "all_comments = get_comments_by_date(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "ticker_dict = {}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    ticker_dict[tick] = {}\n",
    "    \n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        ticker_dict[tick][key] = []\n",
    "        \n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        for com in all_comments[key]:\n",
    "            if tick in com:\n",
    "                ticker_dict[tick][key].append(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1651187912: ['So in other words, long $TSLA',\n",
       "  'So wait… are you saying calls or puts on TSLA?',\n",
       "  'All I heard was buy TSLA',\n",
       "  'So..buy TSLA or die?',\n",
       "  'shitpost, conspiracy, or the gift of prophecy? only time will tell. positions: TSLA, TWTR puts',\n",
       "  'Sooooo…. TSLA is a buy again?',\n",
       "  'While this is a joke post, it’s probably true. He’s said the problem with self driving cars is that you need to solve real world AI first, which is where the robot comes in. The data he can acquire from Twitter could propel that AI in the real world to new heights. \\n\\nI was trying to figure out the play aside from free speech and now I think you have.\\n\\nBullish TSLA',\n",
       "  'So… $TSLA calls it is.',\n",
       "  ' The kind of shitposting that I can get behind.  Puts on TSLA it is because surely Skynet will kill off its maker before anything else.',\n",
       "  'Our only hope is for Jim Cramer to say \"Buy $TSLA!!\"',\n",
       "  'Sir you are at 69 likes, i am not gonna press like that comment to honor the king of the numbers\\n\\nEdit: god damit who ruined the 69 likes? Terminator shall find you and burn you down with the flamethrower made by TSLA',\n",
       "  'TSLA is about to dump!',\n",
       "  'I’m still totally cool with $415, I think TSLA and AMZN will drag the SPY down a ton, especially TSLA now that we have regulatory filing that Musk sold some.',\n",
       "  \"#TSLA . SHOULD'VE BOUGHT MORE\",\n",
       "  'As per his tweet\\n\\n>Elon: No further TSLA sales \"**planned\"** after today\\n\\nStay tuned for unplanned ones ![img](emote|t5_2th52|4275)',\n",
       "  'Tell me he’s not selling shares and tanking the price for funds to buy TWTR on top of what he’s borrowed from a morally questionable Wall Street bank secured against the same stock, while said bank could also be shorting TSLA and could keep going until Musk gets margin called on the debt, crashing it even further and making for an easy profit on top of downside pressure from the macro conditions.\\n\\nBubble go pop? \\U0001fae7📉🔥',\n",
       "  'So I finally bought some TSLA today. Thanks Elon, let’s get it to $1000 so I too can have some change in my pocket.',\n",
       "  'Bro, look at the numbers for domestic automakers selling light truck and commercial EVs.\\n\\nFord GM and Stellantis arent capable of producing more than 20k per year each. Tesla isnt going to be delivering cybertrucks for at least 2 years. Ford isnt even taking orders for new (2022) trucks anymore. GM and Ram are both selling over msrp\\n\\nRivian is sucking dicks behind Wendys but theyre in the same parking lot as everyone else... AND.. theyre vertically integrating major supply chain issues like batteries into their business model. \\n\\nRivian entered the market at an absolutely shit time.. but nobody else can make electric trucks right now either.\\n\\nFuck, we wait months for some parts in the repair industry. The big 3 couldnt supply replacement structural parts a year into the COVID economy.. now we are facing electronics shortages and QC issues in manufacturing.. dont even get me started on Tesla and the fucking parking lots full of broken cars they cant fix and 3-6 month turnarounds for repairs. Its worse than Ford.\\n\\nThe only automaker doing well is TSLA and that shits overvalued meme machine vaporware that only has a pulse because of massive subsidies and a rockstar ceo.']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_dict['TSLA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Comments Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get dates that the comments are from\n",
    "    - two cols, start and end date\n",
    "- Get all of the comments from the dates\n",
    "    - Get the count, and sentiment data from this\n",
    "- Should each ticker have its own df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame()\n",
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "# all_comments = get_comments_by_date(start, end)\n",
    "ticker_dates_dict = {}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    ticker_dates_dict[tick] = pd.DataFrame()\n",
    "\n",
    "# Dates begin yesterday through today:\n",
    "end_day = datetime.today() # End day is today\n",
    "start_day = end_day - timedelta(days=7) # Start day is one week ag\n",
    "\n",
    "# Running for the past week:\n",
    "for tick in tickerlist:\n",
    "    for i in range(0, 7):\n",
    "        end_day = datetime.today() - timedelta(days=i)\n",
    "        start_day = datetime.today() - timedelta(days=i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dates_dict['SPY']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
