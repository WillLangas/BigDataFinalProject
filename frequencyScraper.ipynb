{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Reddit Comment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"p1dG7hgoowK4BSlUdar1WQ\",\n",
    "    client_secret=\"pEePtSnw7KMDZi6fCzkKaOth_pgKpQ\",\n",
    "    password=\"outdoortuesday\",\n",
    "    user_agent=\"Big Data by u/DISWillJayminMaya \",\n",
    "    username=\"DISWillJayminMaya \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def returnListofScores(listofcomments):\n",
    "    templist=[]\n",
    "    for com in listofcomments:\n",
    "        temp = analyzer.polarity_scores(com)\n",
    "        compScore = temp['compound']\n",
    "        templist.append(compScore)\n",
    "    return templist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "subreddit = reddit.subreddit('wallstreetbets')\n",
    "\n",
    "scoreDict={key:list() for key in tickerlist}\n",
    "\n",
    "hot = subreddit.hot(limit=25) # getting first 15 posts in the 'hot' section of the subreddit\n",
    "sum = [0] * len(tickerlist) # our output array\n",
    "counttotal = 0 # total number of comment read\n",
    "submissions_counter = 0\n",
    "\n",
    "rel_comments = [] # List of comments that are relevant to the ticker list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returnCompScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16416/4024818855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[0mrel_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                         \u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturnCompScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                         \u001b[0mscoreDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'returnCompScore' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop for fetching the comments and the amount of time each ticker is mentioned\n",
    "for submissions in hot:\n",
    "    if not submissions.stickied:\n",
    "        submissions_counter+=1\n",
    "        if submissions_counter > 5:\n",
    "            comments = submissions.comments\n",
    "            for comment in comments:\n",
    "                if isinstance(comment, MoreComments):\n",
    "                    continue\n",
    "                counttotal+=1\n",
    "                for i, ticker in enumerate(tickerlist):\n",
    "                    if ticker in comment.body:\n",
    "                        rel_comments.append(comment.body)\n",
    "                        sum[i]=sum[i]+1\n",
    "                        temp = returnCompScore(comment.body)\n",
    "                        scoreDict[ticker].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDict={key:list() for key in tickerlist}\n",
    "spreadDict={key:list() for key in tickerlist}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    if len(scoreDict[tick])>0:\n",
    "        meanDict[tick]=statistics.mean(scoreDict[tick])\n",
    "    if len (scoreDict[tick])>1:\n",
    "        spreadDict[tick]=statistics.stdev(scoreDict[tick])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n",
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n"
     ]
    }
   ],
   "source": [
    "print(meanDict)\n",
    "print(spreadDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments read:  249\n",
      "  Tick  Counts\n",
      "0  GME       1\n"
     ]
    }
   ],
   "source": [
    "output=pd.DataFrame(data={'Tick': tickerlist, 'Counts': sum})\n",
    "print('Total comments read: ',counttotal)\n",
    "print(output[output['Counts']>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Stock Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json, csv\n",
    "\n",
    "size = 'compact' # 'full' for complete historical data, 'compact' for most recent 100\n",
    "ticker = ['GME', 'SPY', 'TWTTR', 'TSLA', 'AMD'] # stock tickers to search for\n",
    "datatype = 'csv' # 'json' for JSON output, 'csv' for CSV output\n",
    "\n",
    "for stock in ticker:\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={stock}&outputsize={size}&datatype={datatype}&apikey=QC1C7LRPUTLC597Q'\n",
    "    response = requests.get(url)\n",
    "    #Save CSV to file\n",
    "    with open(f'{stock}.csv', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-29</td>\n",
       "      <td>88.05</td>\n",
       "      <td>91.790</td>\n",
       "      <td>85.3800</td>\n",
       "      <td>85.52</td>\n",
       "      <td>82647701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>86.67</td>\n",
       "      <td>90.580</td>\n",
       "      <td>84.7800</td>\n",
       "      <td>89.64</td>\n",
       "      <td>91495449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>84.25</td>\n",
       "      <td>87.900</td>\n",
       "      <td>84.0200</td>\n",
       "      <td>84.91</td>\n",
       "      <td>83125054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>89.74</td>\n",
       "      <td>90.120</td>\n",
       "      <td>85.0800</td>\n",
       "      <td>85.16</td>\n",
       "      <td>87805574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>89.86</td>\n",
       "      <td>91.370</td>\n",
       "      <td>88.6100</td>\n",
       "      <td>90.69</td>\n",
       "      <td>93481042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>138.25</td>\n",
       "      <td>139.400</td>\n",
       "      <td>133.4150</td>\n",
       "      <td>133.80</td>\n",
       "      <td>42173963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>141.29</td>\n",
       "      <td>141.365</td>\n",
       "      <td>135.8200</td>\n",
       "      <td>138.55</td>\n",
       "      <td>42224275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>145.16</td>\n",
       "      <td>146.690</td>\n",
       "      <td>137.8000</td>\n",
       "      <td>138.10</td>\n",
       "      <td>53019926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>144.96</td>\n",
       "      <td>147.040</td>\n",
       "      <td>142.7000</td>\n",
       "      <td>145.24</td>\n",
       "      <td>40977478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>143.90</td>\n",
       "      <td>145.760</td>\n",
       "      <td>141.0001</td>\n",
       "      <td>144.85</td>\n",
       "      <td>53359432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp    open     high       low   close    volume\n",
       "0   2022-04-29   88.05   91.790   85.3800   85.52  82647701\n",
       "1   2022-04-28   86.67   90.580   84.7800   89.64  91495449\n",
       "2   2022-04-27   84.25   87.900   84.0200   84.91  83125054\n",
       "3   2022-04-26   89.74   90.120   85.0800   85.16  87805574\n",
       "4   2022-04-25   89.86   91.370   88.6100   90.69  93481042\n",
       "..         ...     ...      ...       ...     ...       ...\n",
       "95  2021-12-13  138.25  139.400  133.4150  133.80  42173963\n",
       "96  2021-12-10  141.29  141.365  135.8200  138.55  42224275\n",
       "97  2021-12-09  145.16  146.690  137.8000  138.10  53019926\n",
       "98  2021-12-08  144.96  147.040  142.7000  145.24  40977478\n",
       "99  2021-12-07  143.90  145.760  141.0001  144.85  53359432\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amd_data = pd.read_csv(\"AMD.csv\")\n",
    "amd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get avg, spread, median of sentiment of comments in any given period\n",
    "    compare this with the performance of the stock\n",
    "\n",
    "model:\n",
    "    based on last x days of reddit comments, what is the price going to be?\n",
    "    take data from x days, put it all into one vector, and predict from this\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a function to return all comments that mention a stock based on a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def make_request(uri, max_retries = 5):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)\n",
    "\n",
    "def get_intervals(startPOSTIX, endPOSTIX, daysInInterval = 1):\n",
    "    \"\"\"\n",
    "    get_intervals goes day by day through the start and end dates, returning that day's POSTIX\n",
    "    \"\"\"\n",
    "    # 86,400 seconds in a day:\n",
    "    period = (86400 * daysInInterval)\n",
    "    end = startPOSTIX + period\n",
    "    \n",
    "    yield(int(startPOSTIX), int(end))\n",
    "    \n",
    "    padding = 1\n",
    "    while end <= endPOSTIX:\n",
    "        startPOSTIX = end + padding\n",
    "        end = (startPOSTIX - padding) + period\n",
    "        yield int(startPOSTIX), int(end)\n",
    "    \n",
    "    \n",
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts(\n",
    "        make_request(URI_TEMPLATE.format\n",
    "                     (subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request( \\\n",
    "                URI_TEMPLATE.format( \\\n",
    "                    subreddit, new_start_at, end_at, bSIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "    return post_collections\n",
    "\n",
    "def get_comments_by_date (startDate, endDate, subreddit='wallstreetbets'):\n",
    "    \"\"\"\n",
    "    Takes a given time interval and scrapes the given subreddit for all of the comments\n",
    "    that relate to the given ticker name, returning them as an array. Basic structure taken \n",
    "    from medium article.\n",
    "    \n",
    "    THIS CURRENTLY WILL NOT WORK IF GIVEN TODAY'S DATE. IT WILL ATTEMPT TO FETCH TOMORROW'S POSTS FOREVER\n",
    "    \"\"\"\n",
    "    # Converting start and end dates to POSTIX:\n",
    "    startDate = math.floor(startDate.timestamp())\n",
    "    endDate = math.floor(endDate.timestamp())\n",
    "\n",
    "    posts = []\n",
    "    # This loop gets all of the posts in the given timeframe\n",
    "    for interval in get_intervals(startDate, endDate):\n",
    "        print(\"-- Fetching Posts From: \", datetime.fromtimestamp(interval[0]), \" to \", datetime.fromtimestamp(interval[1]))\n",
    "        pulled_posts = pull_posts_for(subreddit, interval[0], interval[1])\n",
    "        posts.extend(pulled_posts)\n",
    "#         time.sleep(.100) # So as not to over request reddit\n",
    "\n",
    "    TIMEOUT_SECS = .100\n",
    "    \n",
    "    reddit_posts = []\n",
    "    reddit_comments = {}\n",
    "    reddit_comments[startDate] = []\n",
    "    \n",
    "    # Going through each unique post and comment and adding them to the relevant arrays\n",
    "    #  WARNING: only looking at first 100 posts of each day\n",
    "    for sub_id in np.unique([post['id'] for post in posts])[:10]:\n",
    "        # Only looking at posts with more than 100 upvotes to speed the process up\n",
    "        if reddit.submission(sub_id).ups > 100:\n",
    "            print(\"--- Current Post ID: \", sub_id)\n",
    "            sub = reddit.submission(id=sub_id)\n",
    "            reddit_posts.append(sub)\n",
    "            sub.comments.replace_more(limit=None)\n",
    "            # Looping through each comment:\n",
    "            for comment in sub.comments.list()[:10]: \n",
    "                print(\"---- Current Comment ID: \", comment)\n",
    "                reddit_comments[startDate].append(comment.body)\n",
    "\n",
    "#                 time.sleep(TIMEOUT_SECS)\n",
    "\n",
    "    return reddit_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 11 - Jan 27 2021\n",
    "date_range = []\n",
    "\n",
    "# Filling array with dates (should be 11, 28 for GME Boom)\n",
    "for i in range(11, 28):\n",
    "    date_range.append(datetime(2021, 1, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Fetching Posts From:  2021-01-10 00:00:00  to  2021-01-11 00:00:00\n",
      "-- Fetching Posts From:  2021-01-11 00:00:01  to  2021-01-12 00:00:00\n",
      "-- Fetching Posts From:  2021-01-11 00:00:00  to  2021-01-12 00:00:00\n",
      "-- Fetching Posts From:  2021-01-12 00:00:01  to  2021-01-13 00:00:00\n",
      "--- Current Post ID:  kupcnj\n",
      "---- Current Comment ID:  git71w8\n",
      "---- Current Comment ID:  git7du9\n",
      "---- Current Comment ID:  git9745\n",
      "---- Current Comment ID:  gitbzx8\n",
      "---- Current Comment ID:  git9eky\n",
      "---- Current Comment ID:  git7b0t\n",
      "---- Current Comment ID:  gitil17\n",
      "---- Current Comment ID:  gitj4mj\n",
      "---- Current Comment ID:  git6zh5\n",
      "---- Current Comment ID:  gitvt21\n",
      "-- Fetching Posts From:  2021-01-12 00:00:00  to  2021-01-13 00:00:00\n",
      "-- Fetching Posts From:  2021-01-13 00:00:01  to  2021-01-14 00:00:00\n",
      "--- Current Post ID:  kvellp\n",
      "---- Current Comment ID:  gixt4kw\n",
      "---- Current Comment ID:  gixtbkp\n",
      "---- Current Comment ID:  gixt5mu\n",
      "---- Current Comment ID:  gixtu31\n",
      "---- Current Comment ID:  gixteqm\n",
      "---- Current Comment ID:  gixt0ur\n",
      "---- Current Comment ID:  giy01wi\n",
      "---- Current Comment ID:  giy3yyy\n",
      "---- Current Comment ID:  gixv3w4\n",
      "---- Current Comment ID:  gixtq84\n",
      "-- Fetching Posts From:  2021-01-13 00:00:00  to  2021-01-14 00:00:00\n",
      "-- Fetching Posts From:  2021-01-14 00:00:01  to  2021-01-15 00:00:00\n",
      "--- Current Post ID:  kw35fu\n",
      "---- Current Comment ID:  gj1y3zj\n",
      "---- Current Comment ID:  gj1z1x2\n",
      "---- Current Comment ID:  gj29hn3\n",
      "---- Current Comment ID:  gj1xtdb\n",
      "---- Current Comment ID:  gj243hk\n",
      "---- Current Comment ID:  gj2aphh\n",
      "---- Current Comment ID:  gj20784\n",
      "---- Current Comment ID:  gj20uvk\n",
      "---- Current Comment ID:  gj21hne\n",
      "---- Current Comment ID:  gj221hg\n",
      "-- Fetching Posts From:  2021-01-14 00:00:00  to  2021-01-15 00:00:00\n",
      "-- Fetching Posts From:  2021-01-15 00:00:01  to  2021-01-16 00:00:00\n",
      "--- Current Post ID:  kwscdd\n",
      "---- Current Comment ID:  gj67kxd\n",
      "---- Current Comment ID:  gj6368k\n",
      "---- Current Comment ID:  gj7dapu\n",
      "---- Current Comment ID:  gj6wyz6\n",
      "---- Current Comment ID:  gj714w9\n",
      "---- Current Comment ID:  gj7pr4c\n",
      "---- Current Comment ID:  gj7iiy5\n",
      "---- Current Comment ID:  gj7ovnm\n",
      "---- Current Comment ID:  gj7qy6h\n",
      "---- Current Comment ID:  gj85ab8\n",
      "-- Fetching Posts From:  2021-01-15 00:00:00  to  2021-01-16 00:00:00\n",
      "-- Fetching Posts From:  2021-01-16 00:00:01  to  2021-01-17 00:00:00\n",
      "--- Current Post ID:  kxh5yf\n",
      "---- Current Comment ID:  gja7l9j\n",
      "---- Current Comment ID:  gja7mhv\n",
      "---- Current Comment ID:  gja7p2v\n",
      "---- Current Comment ID:  gja9maf\n",
      "---- Current Comment ID:  gja7l8q\n",
      "---- Current Comment ID:  gja8cms\n",
      "---- Current Comment ID:  gjae0rw\n",
      "---- Current Comment ID:  gjaxixv\n",
      "---- Current Comment ID:  gjb5dy1\n",
      "---- Current Comment ID:  gjbn9ai\n",
      "-- Fetching Posts From:  2021-01-16 00:00:00  to  2021-01-17 00:00:00\n",
      "-- Fetching Posts From:  2021-01-17 00:00:01  to  2021-01-18 00:00:00\n",
      "--- Current Post ID:  ky68ce\n",
      "---- Current Comment ID:  gjeede5\n",
      "---- Current Comment ID:  gjea0oa\n",
      "---- Current Comment ID:  gjeavq9\n",
      "---- Current Comment ID:  gjeambn\n",
      "---- Current Comment ID:  gjehz25\n",
      "---- Current Comment ID:  gjerw5b\n",
      "---- Current Comment ID:  gjf1aqh\n",
      "---- Current Comment ID:  gjeg9ru\n",
      "---- Current Comment ID:  gjejj8p\n",
      "---- Current Comment ID:  gjeu9kn\n",
      "--- Current Post ID:  ky69b4\n",
      "---- Current Comment ID:  gje970c\n",
      "---- Current Comment ID:  gjec7jd\n",
      "---- Current Comment ID:  gjea7y4\n",
      "---- Current Comment ID:  gje9s6t\n",
      "---- Current Comment ID:  gjeazku\n",
      "---- Current Comment ID:  gjebr2c\n",
      "---- Current Comment ID:  gjea7kz\n",
      "---- Current Comment ID:  gjecqcy\n",
      "---- Current Comment ID:  gje99dd\n",
      "---- Current Comment ID:  gjek2ud\n",
      "-- Fetching Posts From:  2021-01-17 00:00:00  to  2021-01-18 00:00:00\n",
      "-- Fetching Posts From:  2021-01-18 00:00:01  to  2021-01-19 00:00:00\n",
      "-- Fetching Posts From:  2021-01-18 00:00:00  to  2021-01-19 00:00:00\n",
      "-- Fetching Posts From:  2021-01-19 00:00:01  to  2021-01-20 00:00:00\n",
      "--- Current Post ID:  kzha9s\n",
      "---- Current Comment ID:  gjntaoz\n",
      "---- Current Comment ID:  gjnst4l\n",
      "---- Current Comment ID:  gjodalj\n",
      "---- Current Comment ID:  gjntlnb\n",
      "---- Current Comment ID:  gjnvtbw\n",
      "---- Current Comment ID:  gjnt3yd\n",
      "---- Current Comment ID:  gjnw4qb\n",
      "---- Current Comment ID:  gjops5o\n",
      "---- Current Comment ID:  gjnt51v\n",
      "---- Current Comment ID:  gjo2m75\n",
      "-- Fetching Posts From:  2021-01-19 00:00:00  to  2021-01-20 00:00:00\n",
      "-- Fetching Posts From:  2021-01-20 00:00:01  to  2021-01-21 00:00:00\n",
      "-- Fetching Posts From:  2021-01-20 00:00:00  to  2021-01-21 00:00:00\n",
      "-- Fetching Posts From:  2021-01-21 00:00:01  to  2021-01-22 00:00:00\n",
      "-- Fetching Posts From:  2021-01-21 00:00:00  to  2021-01-22 00:00:00\n",
      "-- Fetching Posts From:  2021-01-22 00:00:01  to  2021-01-23 00:00:00\n",
      "-- Fetching Posts From:  2021-01-22 00:00:00  to  2021-01-23 00:00:00\n",
      "-- Fetching Posts From:  2021-01-23 00:00:01  to  2021-01-24 00:00:00\n",
      "-- Fetching Posts From:  2021-01-23 00:00:00  to  2021-01-24 00:00:00\n",
      "-- Fetching Posts From:  2021-01-24 00:00:01  to  2021-01-25 00:00:00\n",
      "-- Fetching Posts From:  2021-01-24 00:00:00  to  2021-01-25 00:00:00\n",
      "-- Fetching Posts From:  2021-01-25 00:00:01  to  2021-01-26 00:00:00\n",
      "-- Fetching Posts From:  2021-01-25 00:00:00  to  2021-01-26 00:00:00\n",
      "-- Fetching Posts From:  2021-01-26 00:00:01  to  2021-01-27 00:00:00\n",
      "-- Fetching Posts From:  2021-01-26 00:00:00  to  2021-01-27 00:00:00\n",
      "-- Fetching Posts From:  2021-01-27 00:00:01  to  2021-01-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "all_comments = {}\n",
    "for day in date_range:\n",
    "    end_day = day\n",
    "    start_day = day - timedelta(days=1)\n",
    "    temp_all_coms = get_comments_by_date(start_day, end_day)\n",
    "    all_comments.update(temp_all_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "ticker_dict = {}\n",
    "\n",
    "# Filling ticker_dict with empty dictionaries\n",
    "for tick in tickerlist:\n",
    "    ticker_dict[tick] = {}\n",
    "\n",
    "    # Filling the dictionaries in ticker_dict with empty lists\n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        ticker_dict[tick][key] = []\n",
    "        \n",
    "# Adding the comments to their ticker and date\n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        for com in all_comments[key]:\n",
    "            if tick in com:\n",
    "                ticker_dict[tick][key].append(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1610233200, 1610319600, 1610406000, 1610492400, 1610578800, 1610665200, 1610751600, 1610838000, 1610924400, 1611010800, 1611097200, 1611183600, 1611270000, 1611356400, 1611442800, 1611529200, 1611615600])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GME': {1610233200: [],\n",
       "  1610319600: ['I will become a millionaire by the end of 2021. I will escape the rat race. And I will lose it all on a YOLO GME put \\n\\n&#x200B;\\n\\np: $CMC 88 contracts 26.51 call'],\n",
       "  1610406000: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [\"My broker can't provide me with certain stocks, GME being one of them and I want to cry\",\n",
       "   'So I just joined this sub today, and have seen nothing but GME posts. What happened?\\n\\nEdit: besides GME shooting up. Why did everyone decide to Yolo it? Was there some sort of market indicator?'],\n",
       "  1610665200: [],\n",
       "  1610751600: ['Iâ€™ll FUCKING DO IT AGAIN!!! ðŸš€ðŸš€ðŸ“ˆGME for lifeðŸš€ðŸš€ðŸš€'],\n",
       "  1610838000: [],\n",
       "  1610924400: [],\n",
       "  1611010800: [],\n",
       "  1611097200: [],\n",
       "  1611183600: [],\n",
       "  1611270000: [],\n",
       "  1611356400: [],\n",
       "  1611442800: [],\n",
       "  1611529200: [],\n",
       "  1611615600: []},\n",
       " 'Gamestop': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610406000: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610665200: [],\n",
       "  1610751600: [],\n",
       "  1610838000: [],\n",
       "  1610924400: [],\n",
       "  1611010800: [],\n",
       "  1611097200: [],\n",
       "  1611183600: [],\n",
       "  1611270000: [],\n",
       "  1611356400: [],\n",
       "  1611442800: [],\n",
       "  1611529200: [],\n",
       "  1611615600: []},\n",
       " 'SPY': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610406000: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610665200: [],\n",
       "  1610751600: [],\n",
       "  1610838000: [],\n",
       "  1610924400: [],\n",
       "  1611010800: [],\n",
       "  1611097200: [],\n",
       "  1611183600: [],\n",
       "  1611270000: [],\n",
       "  1611356400: [],\n",
       "  1611442800: [],\n",
       "  1611529200: [],\n",
       "  1611615600: []},\n",
       " 'TWTTR': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610406000: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610665200: [],\n",
       "  1610751600: [],\n",
       "  1610838000: [],\n",
       "  1610924400: [],\n",
       "  1611010800: [],\n",
       "  1611097200: [],\n",
       "  1611183600: [],\n",
       "  1611270000: [],\n",
       "  1611356400: [],\n",
       "  1611442800: [],\n",
       "  1611529200: [],\n",
       "  1611615600: []},\n",
       " 'Twitter': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610406000: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610665200: [],\n",
       "  1610751600: [],\n",
       "  1610838000: [],\n",
       "  1610924400: [],\n",
       "  1611010800: [],\n",
       "  1611097200: [],\n",
       "  1611183600: [],\n",
       "  1611270000: [],\n",
       "  1611356400: [],\n",
       "  1611442800: [],\n",
       "  1611529200: [],\n",
       "  1611615600: []},\n",
       " 'TSLA': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610406000: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610665200: [],\n",
       "  1610751600: [],\n",
       "  1610838000: [],\n",
       "  1610924400: [],\n",
       "  1611010800: [],\n",
       "  1611097200: [],\n",
       "  1611183600: [],\n",
       "  1611270000: [],\n",
       "  1611356400: [],\n",
       "  1611442800: [],\n",
       "  1611529200: [],\n",
       "  1611615600: []},\n",
       " 'Tesla': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610406000: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610665200: [],\n",
       "  1610751600: [],\n",
       "  1610838000: [],\n",
       "  1610924400: [],\n",
       "  1611010800: [],\n",
       "  1611097200: [],\n",
       "  1611183600: [],\n",
       "  1611270000: [],\n",
       "  1611356400: [],\n",
       "  1611442800: [],\n",
       "  1611529200: [],\n",
       "  1611615600: []},\n",
       " 'AMD': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610406000: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610665200: [],\n",
       "  1610751600: [],\n",
       "  1610838000: [],\n",
       "  1610924400: [],\n",
       "  1611010800: [],\n",
       "  1611097200: [],\n",
       "  1611183600: [],\n",
       "  1611270000: [],\n",
       "  1611356400: [],\n",
       "  1611442800: [],\n",
       "  1611529200: [],\n",
       "  1611615600: []}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Comments Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame()\n",
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "# all_comments = get_comments_by_date(start, end)\n",
    "ticker_dates_dict = {}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    ticker_dates_dict[tick] = pd.DataFrame()\n",
    "\n",
    "# Dates begin yesterday through today:\n",
    "end_day = datetime.today() # End day is today\n",
    "start_day = end_day - timedelta(days=7) # Start day is one week ag\n",
    "\n",
    "# Running for the past week:\n",
    "for tick in tickerlist:\n",
    "    for i in range(0, 7):\n",
    "        end_day = datetime.today() - timedelta(days=i)\n",
    "        start_day = datetime.today() - timedelta(days=i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dates_dict['SPY']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
