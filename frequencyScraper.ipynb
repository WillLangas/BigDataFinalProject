{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Reddit Comment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"p1dG7hgoowK4BSlUdar1WQ\",\n",
    "    client_secret=\"pEePtSnw7KMDZi6fCzkKaOth_pgKpQ\",\n",
    "    password=\"outdoortuesday\",\n",
    "    user_agent=\"Big Data by u/DISWillJayminMaya \",\n",
    "    username=\"DISWillJayminMaya \",\n",
    "    prawcore_timeout = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def returnListofScores(listofcomments):\n",
    "    templist=[]\n",
    "    for com in listofcomments:\n",
    "        temp = analyzer.polarity_scores(com)\n",
    "        compScore = temp['compound']\n",
    "        templist.append(compScore)\n",
    "    return templist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "subreddit = reddit.subreddit('wallstreetbets')\n",
    "\n",
    "scoreDict={key:list() for key in tickerlist}\n",
    "\n",
    "hot = subreddit.hot(limit=25) # getting first 15 posts in the 'hot' section of the subreddit\n",
    "sum = [0] * len(tickerlist) # our output array\n",
    "counttotal = 0 # total number of comment read\n",
    "submissions_counter = 0\n",
    "\n",
    "rel_comments = [] # List of comments that are relevant to the ticker list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returnCompScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16416/4024818855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[0mrel_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                         \u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturnCompScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                         \u001b[0mscoreDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'returnCompScore' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop for fetching the comments and the amount of time each ticker is mentioned\n",
    "for submissions in hot:\n",
    "    if not submissions.stickied:\n",
    "        submissions_counter+=1\n",
    "        if submissions_counter > 5:\n",
    "            comments = submissions.comments\n",
    "            for comment in comments:\n",
    "                if isinstance(comment, MoreComments):\n",
    "                    continue\n",
    "                counttotal+=1\n",
    "                for i, ticker in enumerate(tickerlist):\n",
    "                    if ticker in comment.body:\n",
    "                        rel_comments.append(comment.body)\n",
    "                        sum[i]=sum[i]+1\n",
    "                        temp = returnCompScore(comment.body)\n",
    "                        scoreDict[ticker].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDict={key:list() for key in tickerlist}\n",
    "spreadDict={key:list() for key in tickerlist}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    if len(scoreDict[tick])>0:\n",
    "        meanDict[tick]=statistics.mean(scoreDict[tick])\n",
    "    if len (scoreDict[tick])>1:\n",
    "        spreadDict[tick]=statistics.stdev(scoreDict[tick])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n",
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n"
     ]
    }
   ],
   "source": [
    "print(meanDict)\n",
    "print(spreadDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments read:  249\n",
      "  Tick  Counts\n",
      "0  GME       1\n"
     ]
    }
   ],
   "source": [
    "output=pd.DataFrame(data={'Tick': tickerlist, 'Counts': sum})\n",
    "print('Total comments read: ',counttotal)\n",
    "print(output[output['Counts']>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Stock Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json, csv\n",
    "\n",
    "size = 'compact' # 'full' for complete historical data, 'compact' for most recent 100\n",
    "ticker = ['GME', 'SPY', 'TWTTR', 'TSLA', 'AMD'] # stock tickers to search for\n",
    "datatype = 'csv' # 'json' for JSON output, 'csv' for CSV output\n",
    "\n",
    "for stock in ticker:\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={stock}&outputsize={size}&datatype={datatype}&apikey=QC1C7LRPUTLC597Q'\n",
    "    response = requests.get(url)\n",
    "    #Save CSV to file\n",
    "    with open(f'{stock}.csv', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-29</td>\n",
       "      <td>88.05</td>\n",
       "      <td>91.790</td>\n",
       "      <td>85.3800</td>\n",
       "      <td>85.52</td>\n",
       "      <td>82647701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>86.67</td>\n",
       "      <td>90.580</td>\n",
       "      <td>84.7800</td>\n",
       "      <td>89.64</td>\n",
       "      <td>91495449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>84.25</td>\n",
       "      <td>87.900</td>\n",
       "      <td>84.0200</td>\n",
       "      <td>84.91</td>\n",
       "      <td>83125054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>89.74</td>\n",
       "      <td>90.120</td>\n",
       "      <td>85.0800</td>\n",
       "      <td>85.16</td>\n",
       "      <td>87805574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>89.86</td>\n",
       "      <td>91.370</td>\n",
       "      <td>88.6100</td>\n",
       "      <td>90.69</td>\n",
       "      <td>93481042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>138.25</td>\n",
       "      <td>139.400</td>\n",
       "      <td>133.4150</td>\n",
       "      <td>133.80</td>\n",
       "      <td>42173963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>141.29</td>\n",
       "      <td>141.365</td>\n",
       "      <td>135.8200</td>\n",
       "      <td>138.55</td>\n",
       "      <td>42224275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>145.16</td>\n",
       "      <td>146.690</td>\n",
       "      <td>137.8000</td>\n",
       "      <td>138.10</td>\n",
       "      <td>53019926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>144.96</td>\n",
       "      <td>147.040</td>\n",
       "      <td>142.7000</td>\n",
       "      <td>145.24</td>\n",
       "      <td>40977478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>143.90</td>\n",
       "      <td>145.760</td>\n",
       "      <td>141.0001</td>\n",
       "      <td>144.85</td>\n",
       "      <td>53359432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp    open     high       low   close    volume\n",
       "0   2022-04-29   88.05   91.790   85.3800   85.52  82647701\n",
       "1   2022-04-28   86.67   90.580   84.7800   89.64  91495449\n",
       "2   2022-04-27   84.25   87.900   84.0200   84.91  83125054\n",
       "3   2022-04-26   89.74   90.120   85.0800   85.16  87805574\n",
       "4   2022-04-25   89.86   91.370   88.6100   90.69  93481042\n",
       "..         ...     ...      ...       ...     ...       ...\n",
       "95  2021-12-13  138.25  139.400  133.4150  133.80  42173963\n",
       "96  2021-12-10  141.29  141.365  135.8200  138.55  42224275\n",
       "97  2021-12-09  145.16  146.690  137.8000  138.10  53019926\n",
       "98  2021-12-08  144.96  147.040  142.7000  145.24  40977478\n",
       "99  2021-12-07  143.90  145.760  141.0001  144.85  53359432\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amd_data = pd.read_csv(\"AMD.csv\")\n",
    "amd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get avg, spread, median of sentiment of comments in any given period\n",
    "    compare this with the performance of the stock\n",
    "\n",
    "model:\n",
    "    based on last x days of reddit comments, what is the price going to be?\n",
    "    take data from x days, put it all into one vector, and predict from this\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a function to return all comments that mention a stock based on a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def make_request(uri, max_retries = 5):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)\n",
    "\n",
    "def get_intervals(startDate, endDate, daysInInterval = 1):\n",
    "    \"\"\"\n",
    "    get_intervals goes day by day through the start and end dates, returning that day's POSTIX\n",
    "    \"\"\"\n",
    "    # Converting start and end dates to POSTIX:\n",
    "    startPOSTIX = math.floor(startDate.timestamp())\n",
    "    endPOSTIX = math.floor(endDate.timestamp())\n",
    "    # 86,400 seconds in a day:\n",
    "    period = (86400 * daysInInterval)\n",
    "    end = startPOSTIX + period\n",
    "    \n",
    "    yield(int(startPOSTIX), int(end))\n",
    "    \n",
    "    padding = 1\n",
    "    while end <= endPOSTIX:\n",
    "        startPOSTIX = end + padding\n",
    "        end = (startPOSTIX - padding) + period\n",
    "        yield int(startPOSTIX), int(end)\n",
    "    \n",
    "    \n",
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts(\n",
    "        make_request(URI_TEMPLATE.format\n",
    "                     (subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request( \\\n",
    "                URI_TEMPLATE.format( \\\n",
    "                    subreddit, new_start_at, end_at, bSIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "    return post_collections\n",
    "\n",
    "def get_comments_by_date (startDate, endDate, subreddit='wallstreetbets'):\n",
    "    \"\"\"\n",
    "    Takes a given time interval and scrapes the given subreddit for all of the comments\n",
    "    that relate to the given ticker name, returning them as an array. Basic structure taken \n",
    "    from medium article.\n",
    "    \n",
    "    THIS CURRENTLY WILL NOT WORK IF GIVEN TODAY'S DATE. IT WILL ATTEMPT TO FETCH TOMORROW'S POSTS FOREVER\n",
    "    \"\"\"\n",
    "\n",
    "    posts = []\n",
    "    # This loop gets all of the posts in the given timeframe\n",
    "    for interval in get_intervals(startDate, endDate):\n",
    "        print(\"-- Fetching Posts From: \", datetime.fromtimestamp(interval[0]), \" to \", datetime.fromtimestamp(interval[1]))\n",
    "        pulled_posts = pull_posts_for(subreddit, interval[0], interval[1])\n",
    "        posts.extend(pulled_posts)\n",
    "#         time.sleep(.100) # So as not to over request reddit\n",
    "\n",
    "    # Changing time stamps to be readable\n",
    "    \n",
    "    TIMEOUT_SECS = 1\n",
    "    \n",
    "    reddit_posts = []\n",
    "    reddit_comments = {}\n",
    "    startIndex = \"{}-{}-{}\".format(startDate.year, startDate.month, startDate.day)\n",
    "    reddit_comments[startIndex] = []\n",
    "    \n",
    "    # Going through each unique post and comment and adding them to the relevant arrays\n",
    "    #  WARNING: only looking at first 100 posts of each day\n",
    "    for sub_id in np.unique([post['id'] for post in posts])[:100]:\n",
    "        # Only looking at posts with more than 100 upvotes to speed the process up\n",
    "        if reddit.submission(sub_id).ups > 100:\n",
    "            sub = reddit.submission(id=sub_id)\n",
    "            reddit_posts.append(sub)\n",
    "            sub.comments.replace_more(limit=None)\n",
    "            # Looping through each comment:\n",
    "            temp_com_count = 0\n",
    "            for comment in sub.comments.list()[:100]: \n",
    "                temp_com_count += 1\n",
    "                reddit_comments[startIndex].append(comment.body)\n",
    "                \n",
    "            print(\"---- Fethced {} comments from post {}\".format(temp_com_count, sub_id))\n",
    "#             time.sleep(TIMEOUT_SECS)\n",
    "\n",
    "    return reddit_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 11 - Jan 27 2021\n",
    "date_range = []\n",
    "\n",
    "# Filling array with dates (should be 11, 28 for GME Boom)\n",
    "for i in range(11, 28):\n",
    "    date_range.append(datetime(2021, 1, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Fetching Posts From:  2021-01-10 00:00:00  to  2021-01-11 00:00:00\n",
      "-- Fetching Posts From:  2021-01-11 00:00:01  to  2021-01-12 00:00:00\n",
      "---- Fethced 100 comments from post ku2fx9\n",
      "---- Fethced 100 comments from post ku2hgj\n",
      "---- Fethced 37 comments from post ku2kna\n",
      "---- Fethced 100 comments from post ku2ref\n",
      "---- Fethced 57 comments from post ku2ujw\n",
      "---- Fethced 60 comments from post ku2zrg\n",
      "---- Fethced 66 comments from post ku3dkn\n",
      "---- Fethced 100 comments from post ku3ymq\n",
      "---- Fethced 100 comments from post ku400h\n",
      "-- Fetching Posts From:  2021-01-11 00:00:00  to  2021-01-12 00:00:00\n",
      "-- Fetching Posts From:  2021-01-12 00:00:01  to  2021-01-13 00:00:00\n",
      "---- Fethced 53 comments from post kupcnj\n",
      "---- Fethced 100 comments from post kuq6q7\n",
      "---- Fethced 64 comments from post kuqk68\n",
      "---- Fethced 26 comments from post kuqsep\n",
      "---- Fethced 66 comments from post kuqy2m\n",
      "-- Fetching Posts From:  2021-01-12 00:00:00  to  2021-01-13 00:00:00\n",
      "-- Fetching Posts From:  2021-01-13 00:00:01  to  2021-01-14 00:00:00\n",
      "---- Fethced 100 comments from post kvellp\n",
      "---- Fethced 46 comments from post kvf7r1\n",
      "---- Fethced 48 comments from post kvf9od\n",
      "---- Fethced 100 comments from post kvfd4i\n",
      "---- Fethced 100 comments from post kvfgn2\n",
      "---- Fethced 100 comments from post kvgenj\n",
      "-- Fetching Posts From:  2021-01-13 00:00:00  to  2021-01-14 00:00:00\n",
      "-- Fetching Posts From:  2021-01-14 00:00:01  to  2021-01-15 00:00:00\n",
      "---- Fethced 29 comments from post kw35fu\n",
      "---- Fethced 100 comments from post kw36yh\n",
      "---- Fethced 49 comments from post kw38by\n",
      "---- Fethced 73 comments from post kw3gfn\n",
      "---- Fethced 100 comments from post kw3kn2\n",
      "---- Fethced 69 comments from post kw3o3o\n",
      "---- Fethced 100 comments from post kw3ofp\n",
      "---- Fethced 100 comments from post kw4b2p\n",
      "-- Fetching Posts From:  2021-01-14 00:00:00  to  2021-01-15 00:00:00\n",
      "-- Fetching Posts From:  2021-01-15 00:00:01  to  2021-01-16 00:00:00\n",
      "---- Fethced 89 comments from post kwscdd\n",
      "---- Fethced 50 comments from post kwsnz8\n",
      "---- Fethced 100 comments from post kwsuxm\n",
      "---- Fethced 32 comments from post kwsyp0\n",
      "-- Fetching Posts From:  2021-01-15 00:00:00  to  2021-01-16 00:00:00\n",
      "-- Fetching Posts From:  2021-01-16 00:00:01  to  2021-01-17 00:00:00\n",
      "---- Fethced 21 comments from post kxh5yf\n",
      "---- Fethced 25 comments from post kxhag4\n",
      "---- Fethced 45 comments from post kxhdq3\n",
      "---- Fethced 100 comments from post kxhdui\n",
      "---- Fethced 26 comments from post kxhf8l\n",
      "---- Fethced 100 comments from post kxhitu\n",
      "---- Fethced 100 comments from post kxhlxc\n",
      "-- Fetching Posts From:  2021-01-16 00:00:00  to  2021-01-17 00:00:00\n",
      "-- Fetching Posts From:  2021-01-17 00:00:01  to  2021-01-18 00:00:00\n",
      "---- Fethced 48 comments from post ky68ce\n",
      "---- Fethced 97 comments from post ky69b4\n",
      "---- Fethced 100 comments from post ky6eze\n",
      "---- Fethced 49 comments from post ky6gr4\n",
      "---- Fethced 76 comments from post ky6lx7\n",
      "---- Fethced 23 comments from post ky6sdk\n",
      "---- Fethced 37 comments from post ky6u4h\n",
      "---- Fethced 65 comments from post ky6u6s\n",
      "---- Fethced 58 comments from post ky6w9m\n",
      "---- Fethced 78 comments from post ky6ycl\n",
      "-- Fetching Posts From:  2021-01-17 00:00:00  to  2021-01-18 00:00:00\n",
      "-- Fetching Posts From:  2021-01-18 00:00:01  to  2021-01-19 00:00:00\n",
      "---- Fethced 52 comments from post kyu9qq\n",
      "---- Fethced 100 comments from post kyue93\n",
      "---- Fethced 36 comments from post kyunqt\n",
      "---- Fethced 100 comments from post kyuq9h\n",
      "---- Fethced 100 comments from post kyuu13\n",
      "---- Fethced 64 comments from post kyv740\n",
      "---- Fethced 100 comments from post kyvat7\n",
      "---- Fethced 71 comments from post kyvivg\n",
      "---- Fethced 100 comments from post kyvkcz\n",
      "-- Fetching Posts From:  2021-01-18 00:00:00  to  2021-01-19 00:00:00\n",
      "-- Fetching Posts From:  2021-01-19 00:00:01  to  2021-01-20 00:00:00\n",
      "---- Fethced 58 comments from post kzha9s\n",
      "---- Fethced 100 comments from post kzhw2s\n",
      "---- Fethced 41 comments from post kzi26j\n",
      "---- Fethced 25 comments from post kzibdw\n",
      "---- Fethced 64 comments from post kzim6w\n",
      "---- Fethced 24 comments from post kzimtt\n",
      "---- Fethced 100 comments from post kzj3a6\n",
      "-- Fetching Posts From:  2021-01-19 00:00:00  to  2021-01-20 00:00:00\n",
      "-- Fetching Posts From:  2021-01-20 00:00:01  to  2021-01-21 00:00:00\n",
      "---- Fethced 60 comments from post l06cd5\n",
      "---- Fethced 100 comments from post l06dw2\n",
      "---- Fethced 100 comments from post l06n7l\n",
      "---- Fethced 100 comments from post l06psw\n",
      "---- Fethced 100 comments from post l070k3\n",
      "---- Fethced 98 comments from post l07cky\n",
      "---- Fethced 64 comments from post l07etp\n",
      "---- Fethced 100 comments from post l07ly5\n",
      "-- Fetching Posts From:  2021-01-20 00:00:00  to  2021-01-21 00:00:00\n",
      "-- Fetching Posts From:  2021-01-21 00:00:01  to  2021-01-22 00:00:00\n",
      "---- Fethced 21 comments from post l0vwmb\n",
      "---- Fethced 52 comments from post l0wbfx\n",
      "---- Fethced 44 comments from post l0wcfo\n",
      "-- Fetching Posts From:  2021-01-21 00:00:00  to  2021-01-22 00:00:00\n",
      "-- Fetching Posts From:  2021-01-22 00:00:01  to  2021-01-23 00:00:00\n",
      "---- Fethced 62 comments from post l1l1bn\n",
      "---- Fethced 100 comments from post l1l1l8\n",
      "---- Fethced 98 comments from post l1lbhf\n",
      "---- Fethced 26 comments from post l1lifq\n",
      "---- Fethced 64 comments from post l1lnnf\n",
      "---- Fethced 38 comments from post l1lo4x\n",
      "---- Fethced 60 comments from post l1lqxx\n",
      "---- Fethced 20 comments from post l1lvil\n",
      "-- Fetching Posts From:  2021-01-22 00:00:00  to  2021-01-23 00:00:00\n",
      "-- Fetching Posts From:  2021-01-23 00:00:01  to  2021-01-24 00:00:00\n",
      "---- Fethced 44 comments from post l2a742\n",
      "---- Fethced 47 comments from post l2a87f\n",
      "---- Fethced 72 comments from post l2a9vf\n",
      "---- Fethced 52 comments from post l2aamn\n",
      "---- Fethced 100 comments from post l2aeno\n",
      "---- Fethced 100 comments from post l2aklz\n",
      "---- Fethced 30 comments from post l2amdb\n",
      "---- Fethced 25 comments from post l2aqni\n",
      "-- Fetching Posts From:  2021-01-23 00:00:00  to  2021-01-24 00:00:00\n",
      "-- Fetching Posts From:  2021-01-24 00:00:01  to  2021-01-25 00:00:00\n",
      "---- Fethced 16 comments from post l2ze64\n",
      "---- Fethced 100 comments from post l2zf1h\n",
      "---- Fethced 55 comments from post l2zjvl\n",
      "---- Fethced 100 comments from post l2zk5e\n",
      "---- Fethced 95 comments from post l2zlip\n",
      "-- Fetching Posts From:  2021-01-24 00:00:00  to  2021-01-25 00:00:00\n",
      "-- Fetching Posts From:  2021-01-25 00:00:01  to  2021-01-26 00:00:00\n",
      "---- Fethced 100 comments from post l3mtrk\n",
      "---- Fethced 100 comments from post l3mx1d\n",
      "---- Fethced 100 comments from post l3n4sl\n",
      "---- Fethced 72 comments from post l3nb0k\n",
      "-- Fetching Posts From:  2021-01-25 00:00:00  to  2021-01-26 00:00:00\n",
      "-- Fetching Posts From:  2021-01-26 00:00:01  to  2021-01-27 00:00:00\n",
      "---- Fethced 100 comments from post l4afs8\n",
      "-- Fetching Posts From:  2021-01-26 00:00:00  to  2021-01-27 00:00:00\n",
      "-- Fetching Posts From:  2021-01-27 00:00:01  to  2021-01-28 00:00:00\n",
      "---- Fethced 52 comments from post l501ka\n",
      "---- Fethced 31 comments from post l503tw\n",
      "---- Fethced 100 comments from post l504pl\n"
     ]
    }
   ],
   "source": [
    "all_comments = {}\n",
    "for day in date_range:\n",
    "    end_day = day\n",
    "    start_day = day - timedelta(days=1)\n",
    "    temp_all_coms = get_comments_by_date(start_day, end_day)\n",
    "    all_comments.update(temp_all_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "ticker_dict = {}\n",
    "\n",
    "# Filling ticker_dict with empty dictionaries\n",
    "for tick in tickerlist:\n",
    "    ticker_dict[tick] = {}\n",
    "\n",
    "    # Filling the dictionaries in ticker_dict with empty lists\n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        ticker_dict[tick][key] = []\n",
    "        \n",
    "# Adding the comments to their ticker and date\n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        for com in all_comments[key]:\n",
    "            if tick in com:\n",
    "                ticker_dict[tick][key].append(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2021-1-10'])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data = pd.DataFrame.from_dict(ticker_dict)\n",
    "comment_data.to_csv('comment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Comments Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dates_dict['SPY']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
