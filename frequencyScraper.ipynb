{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Reddit Comment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"p1dG7hgoowK4BSlUdar1WQ\",\n",
    "    client_secret=\"pEePtSnw7KMDZi6fCzkKaOth_pgKpQ\",\n",
    "    password=\"outdoortuesday\",\n",
    "    user_agent=\"Big Data by u/DISWillJayminMaya \",\n",
    "    username=\"DISWillJayminMaya \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def returnListofScores(listofcomments):\n",
    "    templist=[]\n",
    "    for com in listofcomments:\n",
    "        temp = analyzer.polarity_scores(com)\n",
    "        compScore = temp['compound']\n",
    "        templist.append(compScore)\n",
    "    return templist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "subreddit = reddit.subreddit('wallstreetbets')\n",
    "\n",
    "scoreDict={key:list() for key in tickerlist}\n",
    "\n",
    "hot = subreddit.hot(limit=25) # getting first 15 posts in the 'hot' section of the subreddit\n",
    "sum = [0] * len(tickerlist) # our output array\n",
    "counttotal = 0 # total number of comment read\n",
    "submissions_counter = 0\n",
    "\n",
    "rel_comments = [] # List of comments that are relevant to the ticker list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returnCompScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16416/4024818855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[0mrel_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                         \u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturnCompScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                         \u001b[0mscoreDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'returnCompScore' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop for fetching the comments and the amount of time each ticker is mentioned\n",
    "for submissions in hot:\n",
    "    if not submissions.stickied:\n",
    "        submissions_counter+=1\n",
    "        if submissions_counter > 5:\n",
    "            comments = submissions.comments\n",
    "            for comment in comments:\n",
    "                if isinstance(comment, MoreComments):\n",
    "                    continue\n",
    "                counttotal+=1\n",
    "                for i, ticker in enumerate(tickerlist):\n",
    "                    if ticker in comment.body:\n",
    "                        rel_comments.append(comment.body)\n",
    "                        sum[i]=sum[i]+1\n",
    "                        temp = returnCompScore(comment.body)\n",
    "                        scoreDict[ticker].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDict={key:list() for key in tickerlist}\n",
    "spreadDict={key:list() for key in tickerlist}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    if len(scoreDict[tick])>0:\n",
    "        meanDict[tick]=statistics.mean(scoreDict[tick])\n",
    "    if len (scoreDict[tick])>1:\n",
    "        spreadDict[tick]=statistics.stdev(scoreDict[tick])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n",
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n"
     ]
    }
   ],
   "source": [
    "print(meanDict)\n",
    "print(spreadDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments read:  249\n",
      "  Tick  Counts\n",
      "0  GME       1\n"
     ]
    }
   ],
   "source": [
    "output=pd.DataFrame(data={'Tick': tickerlist, 'Counts': sum})\n",
    "print('Total comments read: ',counttotal)\n",
    "print(output[output['Counts']>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Stock Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json, csv\n",
    "\n",
    "size = 'compact' # 'full' for complete historical data, 'compact' for most recent 100\n",
    "ticker = ['GME', 'SPY', 'TWTTR', 'TSLA', 'AMD'] # stock tickers to search for\n",
    "datatype = 'csv' # 'json' for JSON output, 'csv' for CSV output\n",
    "\n",
    "for stock in ticker:\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={stock}&outputsize={size}&datatype={datatype}&apikey=QC1C7LRPUTLC597Q'\n",
    "    response = requests.get(url)\n",
    "    #Save CSV to file\n",
    "    with open(f'{stock}.csv', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-29</td>\n",
       "      <td>88.05</td>\n",
       "      <td>91.790</td>\n",
       "      <td>85.3800</td>\n",
       "      <td>85.52</td>\n",
       "      <td>82647701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>86.67</td>\n",
       "      <td>90.580</td>\n",
       "      <td>84.7800</td>\n",
       "      <td>89.64</td>\n",
       "      <td>91495449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>84.25</td>\n",
       "      <td>87.900</td>\n",
       "      <td>84.0200</td>\n",
       "      <td>84.91</td>\n",
       "      <td>83125054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>89.74</td>\n",
       "      <td>90.120</td>\n",
       "      <td>85.0800</td>\n",
       "      <td>85.16</td>\n",
       "      <td>87805574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>89.86</td>\n",
       "      <td>91.370</td>\n",
       "      <td>88.6100</td>\n",
       "      <td>90.69</td>\n",
       "      <td>93481042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>138.25</td>\n",
       "      <td>139.400</td>\n",
       "      <td>133.4150</td>\n",
       "      <td>133.80</td>\n",
       "      <td>42173963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>141.29</td>\n",
       "      <td>141.365</td>\n",
       "      <td>135.8200</td>\n",
       "      <td>138.55</td>\n",
       "      <td>42224275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>145.16</td>\n",
       "      <td>146.690</td>\n",
       "      <td>137.8000</td>\n",
       "      <td>138.10</td>\n",
       "      <td>53019926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>144.96</td>\n",
       "      <td>147.040</td>\n",
       "      <td>142.7000</td>\n",
       "      <td>145.24</td>\n",
       "      <td>40977478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>143.90</td>\n",
       "      <td>145.760</td>\n",
       "      <td>141.0001</td>\n",
       "      <td>144.85</td>\n",
       "      <td>53359432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp    open     high       low   close    volume\n",
       "0   2022-04-29   88.05   91.790   85.3800   85.52  82647701\n",
       "1   2022-04-28   86.67   90.580   84.7800   89.64  91495449\n",
       "2   2022-04-27   84.25   87.900   84.0200   84.91  83125054\n",
       "3   2022-04-26   89.74   90.120   85.0800   85.16  87805574\n",
       "4   2022-04-25   89.86   91.370   88.6100   90.69  93481042\n",
       "..         ...     ...      ...       ...     ...       ...\n",
       "95  2021-12-13  138.25  139.400  133.4150  133.80  42173963\n",
       "96  2021-12-10  141.29  141.365  135.8200  138.55  42224275\n",
       "97  2021-12-09  145.16  146.690  137.8000  138.10  53019926\n",
       "98  2021-12-08  144.96  147.040  142.7000  145.24  40977478\n",
       "99  2021-12-07  143.90  145.760  141.0001  144.85  53359432\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amd_data = pd.read_csv(\"AMD.csv\")\n",
    "amd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get avg, spread, median of sentiment of comments in any given period\n",
    "    compare this with the performance of the stock\n",
    "\n",
    "model:\n",
    "    based on last x days of reddit comments, what is the price going to be?\n",
    "    take data from x days, put it all into one vector, and predict from this\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a function to return all comments that mention a stock based on a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def make_request(uri, max_retries = 5):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)\n",
    "\n",
    "def get_intervals(startPOSTIX, endPOSTIX, daysInInterval = 1):\n",
    "    \"\"\"\n",
    "    get_intervals goes day by day through the start and end dates, returning that day's POSTIX\n",
    "    \"\"\"\n",
    "    # 86,400 seconds in a day:\n",
    "    period = (86400 * daysInInterval)\n",
    "    end = startPOSTIX + period\n",
    "    \n",
    "    yield(int(startPOSTIX), int(end))\n",
    "    \n",
    "    padding = 1\n",
    "    while end <= endPOSTIX:\n",
    "        startPOSTIX = end + padding\n",
    "        end = (startPOSTIX - padding) + period\n",
    "        yield int(startPOSTIX), int(end)\n",
    "    \n",
    "    \n",
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts(\n",
    "        make_request(URI_TEMPLATE.format\n",
    "                     (subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request( \\\n",
    "                URI_TEMPLATE.format( \\\n",
    "                    subreddit, new_start_at, end_at, bSIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "    return post_collections\n",
    "\n",
    "def get_comments_by_date (startDate, endDate, subreddit='wallstreetbets'):\n",
    "    \"\"\"\n",
    "    Takes a given time interval and scrapes the given subreddit for all of the comments\n",
    "    that relate to the given ticker name, returning them as an array. Basic structure taken \n",
    "    from medium article.\n",
    "    \n",
    "    THIS CURRENTLY WILL NOT WORK IF GIVEN TODAY'S DATE. IT WILL ATTEMPT TO FETCH TOMORROW'S POSTS FOREVER\n",
    "    \"\"\"\n",
    "    # Converting start and end dates to POSTIX:\n",
    "    startDate = math.floor(startDate.timestamp())\n",
    "    endDate = math.floor(endDate.timestamp())\n",
    "\n",
    "    posts = []\n",
    "    # This loop gets all of the posts in the given timeframe\n",
    "    for interval in get_intervals(startDate, endDate):\n",
    "        print(\"-- Fetching Posts From: \", datetime.fromtimestamp(interval[0]), \" to \", datetime.fromtimestamp(interval[1]))\n",
    "        pulled_posts = pull_posts_for(subreddit, interval[0], interval[1])\n",
    "        posts.extend(pulled_posts)\n",
    "        time.sleep(.100) # So as not to over request reddit\n",
    "\n",
    "    TIMEOUT_SECS = 1\n",
    "    \n",
    "    reddit_posts = []\n",
    "    reddit_comments = {}\n",
    "    reddit_comments[startDate] = []\n",
    "    \n",
    "    # Going through each unique post and comment and adding them to the relevant arrays\n",
    "    #  WARNING: only looking at first 100 posts of each day\n",
    "    for sub_id in np.unique([post['id'] for post in posts])[:100]:\n",
    "        # Only looking at posts with more than 100 upvotes to speed the process up\n",
    "        if reddit.submission(sub_id).ups > 100:\n",
    "            sub = reddit.submission(id=sub_id)\n",
    "            reddit_posts.append(sub)\n",
    "            sub.comments.replace_more(limit=None)\n",
    "            # Looping through each comment:\n",
    "            temp_com_count = 0\n",
    "            for comment in sub.comments.list()[:100]: \n",
    "                temp_com_count += 1\n",
    "                reddit_comments[startDate].append(comment.body)\n",
    "                \n",
    "            print(\"---- Fethced {} comments from post {}\".format(temp_com_count, sub_id))\n",
    "#             time.sleep(TIMEOUT_SECS)\n",
    "\n",
    "    return reddit_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 11 - Jan 27 2021\n",
    "date_range = []\n",
    "\n",
    "# Filling array with dates (should be 11, 28 for GME Boom)\n",
    "for i in range(11, 28):\n",
    "    date_range.append(datetime(2021, 1, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Fetching Posts From:  2021-01-10 00:00:00  to  2021-01-11 00:00:00\n",
      "-- Fetching Posts From:  2021-01-11 00:00:01  to  2021-01-12 00:00:00\n",
      "--- Current Post ID:  ku2fx9\n",
      "---- Current Comment ID:  gipi7uy\n",
      "---- Current Comment ID:  gipjccd\n",
      "---- Current Comment ID:  giq0m76\n",
      "---- Current Comment ID:  gipjog0\n",
      "---- Current Comment ID:  gipvxwc\n",
      "---- Current Comment ID:  gipxcmx\n",
      "---- Current Comment ID:  giqp8hu\n",
      "---- Current Comment ID:  giptay7\n",
      "---- Current Comment ID:  giq9osz\n",
      "---- Current Comment ID:  giqgosu\n",
      "--- Current Post ID:  ku2hgj\n",
      "---- Current Comment ID:  gipizyq\n",
      "---- Current Comment ID:  gipjqn1\n",
      "---- Current Comment ID:  gipj3xf\n",
      "---- Current Comment ID:  gipo0h2\n",
      "---- Current Comment ID:  gipikpc\n",
      "---- Current Comment ID:  giplbac\n",
      "---- Current Comment ID:  gippzm5\n",
      "---- Current Comment ID:  giq55tw\n",
      "---- Current Comment ID:  gipigw2\n",
      "---- Current Comment ID:  gipjfpl\n",
      "--- Current Post ID:  ku2kna\n",
      "---- Current Comment ID:  gipz0dv\n",
      "---- Current Comment ID:  gipj4d4\n",
      "---- Current Comment ID:  gips1yd\n",
      "---- Current Comment ID:  gipsgzb\n",
      "---- Current Comment ID:  gipiugk\n",
      "---- Current Comment ID:  giqfark\n",
      "---- Current Comment ID:  gipnyak\n",
      "---- Current Comment ID:  gipodre\n",
      "---- Current Comment ID:  gipqqgw\n",
      "---- Current Comment ID:  gitnd8x\n",
      "--- Current Post ID:  ku2ref\n",
      "---- Current Comment ID:  gipncp5\n",
      "---- Current Comment ID:  gipnpit\n",
      "---- Current Comment ID:  gipvygo\n",
      "---- Current Comment ID:  gipk9qk\n",
      "---- Current Comment ID:  gipkcc7\n",
      "---- Current Comment ID:  giporvs\n",
      "---- Current Comment ID:  giqtuul\n",
      "---- Current Comment ID:  gipralb\n",
      "---- Current Comment ID:  gipn3jl\n",
      "---- Current Comment ID:  gipvv2c\n",
      "--- Current Post ID:  ku2ujw\n",
      "---- Current Comment ID:  gipna60\n",
      "---- Current Comment ID:  gipz73f\n",
      "---- Current Comment ID:  gipqrcx\n",
      "---- Current Comment ID:  gips915\n",
      "---- Current Comment ID:  gipw5df\n",
      "---- Current Comment ID:  giptm53\n",
      "---- Current Comment ID:  giplt5z\n",
      "---- Current Comment ID:  giq0svb\n",
      "---- Current Comment ID:  gipr76m\n",
      "---- Current Comment ID:  gippcw8\n",
      "-- Fetching Posts From:  2021-01-11 00:00:00  to  2021-01-12 00:00:00\n",
      "-- Fetching Posts From:  2021-01-12 00:00:01  to  2021-01-13 00:00:00\n",
      "--- Current Post ID:  kupcnj\n",
      "---- Current Comment ID:  git71w8\n",
      "---- Current Comment ID:  git7du9\n",
      "---- Current Comment ID:  git9745\n",
      "---- Current Comment ID:  gitbzx8\n",
      "---- Current Comment ID:  git9eky\n",
      "---- Current Comment ID:  git7b0t\n",
      "---- Current Comment ID:  gitil17\n",
      "---- Current Comment ID:  gitj4mj\n",
      "---- Current Comment ID:  git6zh5\n",
      "---- Current Comment ID:  gitvt21\n",
      "--- Current Post ID:  kuq6q7\n",
      "---- Current Comment ID:  gitk2u0\n",
      "---- Current Comment ID:  giths1s\n",
      "---- Current Comment ID:  gitgp9g\n",
      "---- Current Comment ID:  giu3lw9\n",
      "---- Current Comment ID:  giu3qlz\n",
      "---- Current Comment ID:  gity7wx\n",
      "---- Current Comment ID:  gitw0wo\n",
      "---- Current Comment ID:  gitche6\n",
      "---- Current Comment ID:  gitjy42\n",
      "---- Current Comment ID:  gitkfij\n",
      "--- Current Post ID:  kuqk68\n",
      "---- Current Comment ID:  githl55\n",
      "---- Current Comment ID:  giu5a8d\n",
      "---- Current Comment ID:  gityzo1\n",
      "---- Current Comment ID:  giu0vlw\n",
      "---- Current Comment ID:  gituudv\n",
      "---- Current Comment ID:  giu9rvh\n",
      "---- Current Comment ID:  giu3v5v\n",
      "---- Current Comment ID:  gitk3xh\n",
      "---- Current Comment ID:  gitgv5p\n",
      "---- Current Comment ID:  giu4n89\n",
      "-- Fetching Posts From:  2021-01-12 00:00:00  to  2021-01-13 00:00:00\n",
      "-- Fetching Posts From:  2021-01-13 00:00:01  to  2021-01-14 00:00:00\n",
      "--- Current Post ID:  kvellp\n",
      "---- Current Comment ID:  gixt4kw\n",
      "---- Current Comment ID:  gixtbkp\n",
      "---- Current Comment ID:  gixt5mu\n",
      "---- Current Comment ID:  gixtu31\n",
      "---- Current Comment ID:  gixteqm\n",
      "---- Current Comment ID:  gixt0ur\n",
      "---- Current Comment ID:  giy01wi\n",
      "---- Current Comment ID:  giy3yyy\n",
      "---- Current Comment ID:  gixv3w4\n",
      "---- Current Comment ID:  gixtq84\n",
      "--- Current Post ID:  kvf7r1\n",
      "---- Current Comment ID:  gixxlmv\n",
      "---- Current Comment ID:  gixwxe6\n",
      "---- Current Comment ID:  giy6gyc\n",
      "---- Current Comment ID:  gixxr7i\n",
      "---- Current Comment ID:  giy8tjq\n",
      "---- Current Comment ID:  gixyaxk\n",
      "---- Current Comment ID:  giy8unl\n",
      "---- Current Comment ID:  giypiit\n",
      "---- Current Comment ID:  giywojl\n",
      "---- Current Comment ID:  giyzqp9\n",
      "--- Current Post ID:  kvf9od\n",
      "---- Current Comment ID:  gixxbm4\n",
      "---- Current Comment ID:  giy0wtq\n",
      "---- Current Comment ID:  gixxqfc\n",
      "---- Current Comment ID:  giy7f7w\n",
      "---- Current Comment ID:  giy1dh2\n",
      "---- Current Comment ID:  giy2qb1\n",
      "---- Current Comment ID:  giyfzrv\n",
      "---- Current Comment ID:  giz6yi2\n",
      "---- Current Comment ID:  gl95k4r\n",
      "---- Current Comment ID:  giyjl9o\n",
      "--- Current Post ID:  kvfd4i\n",
      "---- Current Comment ID:  giy0qnx\n",
      "---- Current Comment ID:  gixxmrp\n",
      "---- Current Comment ID:  giy1wuv\n",
      "---- Current Comment ID:  gixxq1v\n",
      "---- Current Comment ID:  gixz2nl\n",
      "---- Current Comment ID:  giyo3ab\n",
      "---- Current Comment ID:  giyafr2\n",
      "---- Current Comment ID:  gixzbs6\n",
      "---- Current Comment ID:  giy5b4b\n",
      "---- Current Comment ID:  giy0c8f\n",
      "--- Current Post ID:  kvfgn2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8688/3448900850.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mend_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mday\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtemp_all_coms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_comments_by_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_day\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mall_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_all_coms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8688/1353675359.py\u001b[0m in \u001b[0;36mget_comments_by_date\u001b[1;34m(startDate, endDate, subreddit)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msub_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mreddit_posts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace_more\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[1;31m# Looping through each comment:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\models\\comment_forest.py\u001b[0m in \u001b[0;36mreplace_more\u001b[1;34m(self, limit, threshold)\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0mnew_comments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mremaining\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\models\\reddit\\more.py\u001b[0m in \u001b[0;36mcomments\u001b[1;34m(self, update)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;34m\"sort\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment_sort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             }\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPI_PATH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"morechildren\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, data, files, params, json)\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mattempts\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m                 return self._objectify_request(\n\u001b[0m\u001b[0;32m    794\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m                     \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36m_objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    694\u001b[0m         \"\"\"\n\u001b[0;32m    695\u001b[0m         return self._objector.objectify(\n\u001b[1;32m--> 696\u001b[1;33m             self.request(\n\u001b[0m\u001b[0;32m    697\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At most one of `data` and `json` is supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m             return self._core.request(\n\u001b[0m\u001b[0;32m    886\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"api_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         return self._request_with_retries(\n\u001b[0m\u001b[0;32m    331\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mretry_strategy_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         response, saved_exception = self._make_request(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    183\u001b[0m     ):\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             response = self._rate_limiter.call(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"headers\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\rate_limit.py\u001b[0m in \u001b[0;36mdelay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Sleeping: {sleep_seconds:0.2f} seconds prior to call\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep_seconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse_headers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_comments = {}\n",
    "for day in date_range:\n",
    "    end_day = day\n",
    "    start_day = day - timedelta(days=1)\n",
    "    temp_all_coms = get_comments_by_date(start_day, end_day)\n",
    "    all_comments.update(temp_all_coms)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Fetching Posts From:  2021-01-16 00:00:00  to  2021-01-17 00:00:00\n",
      "-- Fetching Posts From:  2021-01-17 00:00:01  to  2021-01-18 00:00:00\n",
      "--- Current Post ID:  ky68ce\n",
      "---- Fethced 48 comments from post\n",
      "--- Current Post ID:  ky69b4\n",
      "---- Fethced 97 comments from post\n",
      "--- Current Post ID:  ky6eze\n",
      "---- Fethced 100 comments from post\n",
      "--- Current Post ID:  ky6gr4\n",
      "---- Fethced 49 comments from post\n",
      "--- Current Post ID:  ky6lx7\n",
      "---- Fethced 76 comments from post\n",
      "--- Current Post ID:  ky6sdk\n",
      "---- Fethced 23 comments from post\n",
      "--- Current Post ID:  ky6u4h\n",
      "---- Fethced 37 comments from post\n",
      "--- Current Post ID:  ky6u6s\n",
      "---- Fethced 65 comments from post\n",
      "--- Current Post ID:  ky6w9m\n",
      "---- Fethced 58 comments from post\n",
      "--- Current Post ID:  ky6ycl\n",
      "---- Fethced 78 comments from post\n"
     ]
    }
   ],
   "source": [
    "# Change this to the next day:\n",
    "day = date_range[6]\n",
    "end_day = day\n",
    "start_day = day - timedelta(days=1)\n",
    "temp_all_coms = get_comments_by_date(start_day, end_day)\n",
    "all_comments.update(temp_all_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1610233200, 1610319600, 1610492400, 1610578800, 1610751600])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "ticker_dict = {}\n",
    "\n",
    "# Filling ticker_dict with empty dictionaries\n",
    "for tick in tickerlist:\n",
    "    ticker_dict[tick] = {}\n",
    "\n",
    "    # Filling the dictionaries in ticker_dict with empty lists\n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        ticker_dict[tick][key] = []\n",
    "        \n",
    "# Adding the comments to their ticker and date\n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        for com in all_comments[key]:\n",
    "            if tick in com:\n",
    "                ticker_dict[tick][key].append(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1610233200, 1610319600, 1610492400, 1610578800, 1610751600])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GME': {1610233200: [\"Mr. Beast definitely has the funds to take a nice stake in GME. Wonder if he's a gambling man? Oh wait, he just bought a **million** dollars worth of scratch off tickets. Someone tweet the man the DD.\"],\n",
       "  1610319600: ['I will become a millionaire by the end of 2021. I will escape the rat race. And I will lose it all on a YOLO GME put \\n\\n&#x200B;\\n\\np: $CMC 88 contracts 26.51 call'],\n",
       "  1610492400: ['Inorganic brand engagement: An autistic way\\n\\nI see, people creating memes and post to encourage herd buying of GME shares. In fact, I joined in myself. \\n\\nI was thinking if people could boost brand engagements of a company in a similar way. The ticker is GOEV. What if 10-20% people on this sub decided to follow and engage with Canoo Incâ€™s social media accounts/posts e.g. following their page, liking and commenting their posts. This could provide a 5-10 fold increase in their engagement rate. Better brand engagement would mean better awareness of the company and their product. A retarded way to think would be less advertising expense which means more money for R&D. MoreðŸ’° in R&D- better product- higher stock prices.\\n\\nIf this is not deleted my mods, go and follow Canoo Inc. in Instagram, Twitter, LinkedIn etc. Like and comment. GOEV to the moon ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       "   'Tired of GME went into GOEV today  \\n\\n\\nEVs making me precum',\n",
       "   'Have you ever cried tears of joy? Jumped for joy? These were the emotions I was feeling yesterday when I heard Captain Cohen and two of his finest wingmen had joined the board of GME for it means one thing: TENDIES. SECURED.',\n",
       "   'GME fanlevel is almost on TSLA level rn',\n",
       "   'Thank you, may the tendies be ever in your favor. GME ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       "   \"So much nostalgia and GME hype in one video. I can't take it. Buying more shares.\",\n",
       "   'The ending clip with Ryan Cohen was a classy touch. GME to the moon ðŸš€',\n",
       "   'GME is going fly with iron wings unlike that retard icarus',\n",
       "   'yes, even just last night I was thinking, wait did they just fly from the US across the Atlantic to drop bombs in the middle east?\\n\\nIts all an analogy, when someone notices GME and the potential = Target Acquired\\n\\nFund managers who have failed to hedge their shorts effectively = storing explosives and fuel in the control tower\\n\\nIneffective MiGs = inept fund managers who have had years to get into the correct position, but they are still loitering deep in short-istan.',\n",
       "   \"Awesome! Ryan Cohen's Gamestop is a completely different animal (get it? GET IT?)\\n\\nGME Gang lets gooooooo!!!!!!!!\\n\\n28k shares here across 3 accounts\",\n",
       "   'You need to up your position size. I was once at 101% on GME.',\n",
       "   'The legacy stock is a meme for sure, but the Ryan Cohen GME is definitely not a meme.',\n",
       "   \"Since RC entered game it's not meme stock anymore. Before it goes under his entry price, and it will happen never or years from now, but never, it will get to $30 end of Q1 2021 - almost 100% gain. How fast to the moon from there I have no clue. If squeeze happens - well...\\n\\nGME is a safe harbour for rockets. LONG TERM.\",\n",
       "   'Thinks GME is a meme stock, oh yeah youâ€™re in the right place',\n",
       "   'In autist language: Because. I. Fuck.\\n\\nIn English: My 12-month PT is $32. I arrived at that by assuming they only generate half the FCF they did last cycle, adjusted for inflation, and assumed their gross margin would suffer by 20%. Thatâ€™s it. Now if that seems like the worst case scenario for $GME next year then I think youâ€™d be about right. So actually, this meme is magic b/c of the margin of safety built-in. Take any kind of bullish skew on growth, new business lines, and e-commerce multiples, etc... and you get a bigger number.\\n\\nI hope that helps.',\n",
       "   \"Completely agree. I'm 70% GME, other 30% is in ICLN shares + leaps.\",\n",
       "   \"100% agree.\\n\\n1. Covid play, when lockdown lifts their brick and mortar back in biz.\\n2. Activist Cohen wants in and is an amazing turn around stock. Brick/Motar -> E-commerce\\n3. Reverse bubble, only thing popping is Melvin's wife when I'm done with her. 71.2M shares needs to cover at some point cuz they're bleeding. 71.2M shares in buys guaranteed by June when they recall for board voting.\\n4. Console cycle tendies.\\n\\nDownside? Gay manipulation idc I'm holding shares, these don't expire.\\n\\nPeople don't understand playing the market is playing with risk. You can make a fuck ton elsewhere buying FD and hoping you make a 10 bagger but GME is one of those once in a lifetime opportunity that doesn't happen often where risk is 0 and reward is insane.\",\n",
       "   'What were some of the main positions you sold down to buy more GME? This looks like it might be TD, is that average cost in CAD or USD?',\n",
       "   \"Please research PPC advertising and other digital marketing channels (in which Cohen has extensive experience in), and then get back to me. That's just one of many ways to substantially grow GME's revenue and profits. You don't need to invent the wheel to be a very successful business. I'm in digital marketing and made $400K in 2019. I don't do anything special like Tesla.\",\n",
       "   'Now that we have his attention....  vote GME',\n",
       "   'I wanna see this play out except he buys all GME shares ðŸ˜‚\\n\\nObligatory \\nðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       "   '100k to GME, infinity squeeze tendies to charity. Fuck these tards here they donâ€™t need 20%.\\n\\nLFG u/MrBeast100kinvest ðŸš€ðŸš€ðŸš€',\n",
       "   'Can you imagine if mr Beast made a video about the GME short squeeze?',\n",
       "   'Plot twist...Mr. Beast has been positioning GME all along.',\n",
       "   'GME',\n",
       "   'Yeah but we want him to purchase GME shares now. Soooo lets get him back here to purchase GME shares so everyone can benefit.',\n",
       "   \"Im so disappointed at you guys. So we have an opportunity to make some millionaire YouTube loose 100k, but you people are more worried about your GME and PLTR calls??? Is this /investing? \\n\\nMake him buy far OTM SIGL calls or NKLA for that purpose.  Loss porn before you people's gains.\",\n",
       "   'GME for certain, many of our brethren are being shot down left and right because we cannot maintain a steady conquest',\n",
       "   'get him to buy 1 million dollar worth of GME',\n",
       "   '$REEEEE aka $GME',\n",
       "   'Dear Mr Beast sir, Pleas buy GME. $100k will make the whales notice and stir up the big squeee',\n",
       "   'Mr Beast you have been summoned.\\n\\nDrop 100k on GME shares. The little company who is rising like a phoenix.',\n",
       "   'Make him spend it all on GME shares. Then his army of 50 million subscribers or whatever it is will hop on board with 1 share each',\n",
       "   \"The crazy thing is 5k shares of GME wouldn't even qualify him as one of the  GME whales. That's like upper middle class when it comes to GME\",\n",
       "   'GME $30 EOW ðŸš€ðŸš€',\n",
       "   \"I have no idea what anything on this sub means.\\n\\nBut after watching a YT video on this community, I joined the subreddit. \\n\\n45 minutes later, I downloaded RH. \\n\\nOne stimulus and 3 weeks later, I have 20 shares of $GME and 8 shares of $PLTR. \\n\\nI invest more every week and forget that I'm saving $ for the first time in 5 years.\\n\\nAll the charts are green so I'm happy. \\n\\nThank you, $Autists.\",\n",
       "   '>when you say \"technical analysis\" I hear Tech, $NKLA, and anal cysts.\\n\\n\\nI died lol\\n\\n\\nalso, RTRD - yes is gold \\n\\n\\nGME ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       "   '**I am a bot and identified and tracked the following options picks within this post:**\\n\\n|**Ticker**|**Strike**|**Type**|**Exp**|**Recorded Premium**|**Recorded Stock Price**|**OI**|**Volume**|\\n|:-|:-|:-|:-|:-|:-|:-|:-|\\n|**GME**|**$20.5**|**BUY CALL**|**2021-01-29**|**$1.9**|**$19.95**|**30**|**64**\\n|**GME**|**$20**|**BUY CALL**|**2021-07-16**|**$6.48**|**$19.95**|**3330**|**192**\\n\\n[Realtime ROI](https://www.pickmojo.com/pick/ckjun0uek1bxs0788ofwhpzul) | [Track Record](https://www.pickmojo.com/user/unnovator/) | [Bot Info](https://www.reddit.com/r/pickmojo/comments/he9ghu/options_picks_tracking_bot_information/) | Leaderboard: [Week](https://www.pickmojo.com/stocks/best-this-week-reddit), [Month](https://www.pickmojo.com/stocks/best-this-month-reddit), [All](https://www.pickmojo.com/stocks/top-reddit) | [Exit this position](https://www.pickmojo.com/reddit_redirect)\\n\\n**^(*Recorded after market close, will be recorded at the next market open if the premium is within 10% margin. My owner is monitoring these posts, reply with feedback! You can now track comments by mentioning me!)**',\n",
       "   'Better bet than GME!',\n",
       "   'If game stop grows much more papa Karp is gonna draft up a contract for GME lmao ðŸš€',\n",
       "   'Sell it and buy GME'],\n",
       "  1610578800: [\"My broker can't provide me with certain stocks, GME being one of them and I want to cry\",\n",
       "   'So I just joined this sub today, and have seen nothing but GME posts. What happened?\\n\\nEdit: besides GME shooting up. Why did everyone decide to Yolo it? Was there some sort of market indicator?',\n",
       "   'I feel the same way. Saw all the GME posts, but never bought.',\n",
       "   'I thought It was a retarded play. Then me and my buddy started looking at revenues and thought holy shut these retards might be on to something. Then two days later GME pops off... ðŸ¤¯',\n",
       "   'Years of blood sweat and tears son. \\nAll for this moment, let them celebrate. Go look through some older posts or the $GME chart if this is already to much reading.',\n",
       "   \"I just spent 45 minutes of scrolling and just keep seeing rocket ships and GME memes. It boils down to a guy named Deep Value/Roaring Kitty but I haven't yet watched his videos.\\n\\nWas just curious what normally starts these trends but I'm gonna lurk for a while more and follow Jack a lot and Deep Value and see how it goes.\",\n",
       "   \"10% down from credit card cash withdrawal or mom's 401k, YOLO rest on GME.\\n\\nMiliionair with a house. Cantgotitsup.\",\n",
       "   \"See if you can find their older posts. Cliff's Notes:\\n\\nGME has been shorted to death over the past few years with the conventional wisdom saying that it was essentially going bankrupt being a brick and mortar retailer who's main product has been migrating to online downloads.\\n\\nHowever the DD essentially showed that the stock was way undervalued just based on cash on hand and expected revenues alone. There was no way in hell it would go bankrupt any time soon. Furthermore the average gamer does not have the internet speeds needed to go 100% digital only. Physical purchases will still be required for some time.\\n\\nMichael Burry (Big Short fame) hopped in for a huge chunk with a contrarian play amidst the short frenzy. Ryan Cohen (founder of Chewy) then hopped in as an activist investor and has continued loading up to over 10% for months. The board played a game of chicken with him lining up a shelf offering, but proceeded to flinch after he upped his stake even further.\\n\\nAs all of this has been happening the stock was mooning from the $3-$5 range up to the mid teens and low 20s these past few months, with some nose bleed dips in between. Microsoft has been revealed to have a revenue sharing partnership with GME where GME gets a cut of future digital downloads made on XBoxes originally purchased at GME.\\n\\nThe console cycle hit, and it's clear that alone will be enough to let GME at worse tread water for another 6 months as they allegedly start transitioning to a much larger online presence. Game resale continues to be a decent revenue stream as well.\\n\\nAll the while, short interest has not really let up as of December 31st. \\n\\nRyan Cohen was just recently offered a spot on the board this week and that may have been the final straw for some of these major short players. He turned Chewy into an online phenomenon this year and people are essentially shook that he will take over as an executive at Game Stop and get them headed in the same direction.\\n\\nBottom line, for at least a fraction of shorters the bankruptcy thesis is now blown up and they're looking for a way out without too much bleeding. Melvin Capital was allegedly averaging a borrow price of around $24 per share, but we just blew right past that yesterday.\\n\\nThere are more shorts borrowed then shares in existence right now, and a lot of big dogs are not willing to part ways with them for anything under triple digits a share. So if panic sets in among the shorts we could see a cascade of buying to close positions and the squeeze will officially be squozen.\\n\\nThis is the theory anyhow.\",\n",
       "   'A new age of humanity? Finally a smart person in charge? This guy will take us to the red planet? Probably...\\n\\nBUT FIRST, HIJACKING YOUR POST:\\n\\nBUY GME ðŸš€ðŸš€ðŸš€ (and TSLA for sure for Elon, BUT FIRST GME ðŸš€ðŸš€ðŸš€)',\n",
       "   'I 0kedge that my next pay slip will be put into GME',\n",
       "   'lol Iâ€™m curious to know how much somebody couldâ€™ve lost buying naked puts on GME before today. \\n\\nPenance for being ðŸŒˆ ðŸ»',\n",
       "   'GME will take care of that for you, autist',\n",
       "   'Iâ€™m literally telling everyone I know to buy GME shares right now. (Well, been telling people but now theyâ€™re listening)\\n\\n\\nCant go tits up!\\n\\nðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       "   'https://nakedshortreport.com/company/GME\\n\\nEdit-short interest INCREASED today!',\n",
       "   'Buy $GME',\n",
       "   'No but this is daily\\n\\nhttps://nakedshortreport.com/company/GME',\n",
       "   'https://nakedshortreport.com/company/GME\\n\\nðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       "   'https://nakedshortreport.com/company/GME',\n",
       "   'Short interest increased today! Spread the word!\\nhttps://nakedshortreport.com/company/GME',\n",
       "   \">https://nakedshortreport.com/company/GME\\n\\nYou think this means there is still quite a bit of shorts that haven't covered and that the squeeze has not squoze?\",\n",
       "   'This may be true. I just posted this to help a good cause. I didnâ€™t take the picture. I donâ€™t have a Bloomberg terminal. Heck, I canâ€™t even read. \\nAll I know is GME ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€'],\n",
       "  1610751600: ['Iâ€™ll FUCKING DO IT AGAIN!!! ðŸš€ðŸš€ðŸ“ˆGME for lifeðŸš€ðŸš€ðŸš€',\n",
       "   \"First rule of $GME club, you don't sell at $GME club.\",\n",
       "   \"I'll let you know how trying to use open source software goes, I'm wanting to make some of these memes too but I would rather buy another $GME than proprietary software\",\n",
       "   'I need someone to explain to my wife that our life savings belongs in GME.. also I need a place stay tonight.',\n",
       "   \"He lives in a cardboard box under the slide at McDonald's because he shorted GME so...\",\n",
       "   \"Shhhhh. Don't tell anyone until we make 100x on GME and can open larger BB positions\",\n",
       "   'No idea about GME posts being deleted but I think you will find a lot of people were eyeing BB but it only became popular over the last few days because of the large price movement.',\n",
       "   'Definitely not GME shares',\n",
       "   \"$364...that's enough to buy some GME\",\n",
       "   'Imagine not buying GME',\n",
       "   'I sold 20k shares of GME last Friday after I had held for 2 months.',\n",
       "   \"Well I saw another play with Magnite, and I wanted a bigger pot to buy back into GME. I actually doubled my 10k to 20k from it. But I couldn't use margin like I could with shares for GME (thats how I had 20k in). So I only missed out on 10k.\",\n",
       "   'GME GANG HAD THEIR FUN????????? \\n\\nMUTHAFUKER DO YOU THINK THIS SHOW IS EVEN CLOSE TO OVER',\n",
       "   'GME had their fun? This is just getting started',\n",
       "   \"Once Beiber posts a picture from space, that's when it's too late to get in. Right now is still pretty good at $30\\n\\nHow many of the GME gang are reserving seats now that they can afford it? Agreed, it's still not too late for people that aren't in. (100 at 18.51, should have bought more).  Didn't the aborted flight with a safe landing cross another requirement off the FAA list?\",\n",
       "   'GME isnâ€™t done idiot but I do agree on SPCE',\n",
       "   'I rolled my space calls yesterday @ 32.5 into more GME. Everything but GME is immaterial for the next few weeks. Rip',\n",
       "   \"Fuck BB and SPCE. It's GME's time. They can come later.\",\n",
       "   'GME prints next week!  Donâ€™t cheapen your fellow degens',\n",
       "   \"You make me look like I'm smort. ðŸ˜€\\n   BB at 7.20\\nGME at 8.52\\nSPCE at 19.35\\n\\nIf only I had thrown real money at it not pocket change.\",\n",
       "   'Mandatory: GMEðŸš€ðŸš€ðŸš€ðŸŒ•',\n",
       "   'Iâ€™m on this retarded train with all you fuckers! These memes are worth every penny of my GME investment, even if I lose! GME to the fucking noon!!! ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ’ŽðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼ðŸ™ŒðŸ¼',\n",
       "   'Buy GME and DIAMONDðŸ’ŽðŸ™Œ HAND Retards becaus WE ARE GOING TO THE MOON ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       "   'Mandatory: GMEðŸš€ðŸš€ðŸš€ðŸŒ•',\n",
       "   'Alright, this convinced me to convert at least some of my portfolio to GME shares Monday. Star wars dubs have a special place in my heart',\n",
       "   \"The prequals are still bad.\\n\\nBut GME isn't. Currently taking out outrageous loans to buy more.\",\n",
       "   'Everyone is retarded here, if you say buy GME they will buy Faggy Ds, make sure you say \"BUY SHARES ON GME\" that is the only thing that will get us there. Not options',\n",
       "   'I see my number there. RIP GME 40c 1/15 ðŸ’Žâœ‹',\n",
       "   'You mf. Im taking my GME calls with me to my grave',\n",
       "   'Mandatory: GMEðŸš€ðŸš€ðŸš€ðŸŒ•',\n",
       "   'Hey I have a serious question about margin. I have a Fidelity with 30k in equities. I was so so close to joining in on the LMND gamma squeeze on Jan 7th on exactly 1:04pm but I didnâ€™t pull the trigger. I had to drive to work. I had an intraday buying power of 85k. Question is had I went maximum leverage and held for one week, what could have happened??\\n\\nPlease help me understand this. On the most basic level, from what Iâ€™m understanding, if I buy at an entry point that is a â€œbottomâ€ moving forward for a swing trade ... is that a successful trade as long as the price doesnâ€™t go below where I buy it from?\\n\\nMy reasoning behind LMND @ 1pm that day was Fool was teasing their buy and revealed it was LMND and they pre-teased it as a 2021 stock, plus it was already being heavily shorted and carried high option interest. If you look at the charts with BBands and 21 EMA, it was like buying at the bottom of a massive hill.\\n\\nLetâ€™s say it does go negative on me for 5min or 3 hours but if I hold I end the day up 1% and the next day 7%, will I get a margin call within minutes or hours in the same day?\\n\\nThank you ðŸ™ðŸ½ ðŸ’Ž thank you\\n\\nOf course knowing this knowledge will grant me a level 30+ boosting weapon because next time I see a wave of opportunity like I saw looking at GME @ $40, I wouldâ€™ve pulled the trigger...the max fucking trigger because itâ€™s all or nothing. I have nothing, not even a proper bed, and I have people I need to give to and I know Iâ€™m intelligent enough to catch a GME with DD and patience. Please help me understand. I understand thereâ€™s requirements that can altered but not fully the terms & agreements.\\n\\nThat ~90k couldâ€™ve 3x-4x into a quarter milli\\nThe LMND couldâ€™ve netted me $30k\\n\\nI need to shake these gun shy hands.',\n",
       "   \"I did the exact opposite..\\n\\nYOLO'd everything into purchase 2680 shares of GME  @41.34  during pre market.  My paper hands sold it at almost the effing bottom at $34.20..   $17850 evaporated lost all my gains since November.  \\n\\n\\nI Then took the remains and purchased  AMD. and some calls for VALE 18 Jun 20C\\n\\n\\nCame home from work and  found out I really f'd up and should have not sold.... repurchase shares in my long term account at ETrade and my webull account...I was able to scrape together 1/3 of what I had this morning\\n\\n.....I foolishly paper handed a lambo  buyt may be able to get a rusted out Honda with this combo...\",\n",
       "   'I had a job interview  at 11am  I get out  at noon.. I see GME is  down 12% and I panic.... what can I say.....',\n",
       "   \"12% is normal for GME, then it'll do single digit loses, then it'll make it all back and then some in a day.\",\n",
       "   'welcome to GME World on WSB channel',\n",
       "   'WSB was literally talking about two stocks only this week: GME and BB. Shares or calls any day before today would have made you money.',\n",
       "   'Well if you invested in the stock people were actually talking about (GME) you would be doing a whole lot better.',\n",
       "   \"Dumb fuck. We're doing GME this week, not TSLA\",\n",
       "   \"You're home retard! Now do one more yolo on GME ðŸš€ðŸš€ðŸš€ðŸš€\",\n",
       "   'in 3 months youll see the wildest V shaped recovery of your life, BB to the Moon, GME will kill Melvin Capital, Destroy all institutional investors!\\n\\n&#x200B;\\n\\nWSB OR DIE',\n",
       "   'You started last week and donâ€™t have any GME/BB? You are a retard',\n",
       "   'If you found this sub last week then you should have BALLS deep in GME shares. Man, the amount of retards on this subreddit. Iâ€™ve made what you lost this week holding 100 GME! ðŸ¤·ðŸ»\\u200dâ™‚ï¸']},\n",
       " 'Gamestop': {1610233200: [\"You have some hope next week after Gamestops ICR presentation next Monday 4:30 EST. Maybe Tuesday the stock sees a jump. Cohen tweets seem bullish in a trolly way. But yeah you're prob fucked LMAO. Might as well hold until then.\"],\n",
       "  1610319600: [],\n",
       "  1610492400: [\"Awesome! Ryan Cohen's Gamestop is a completely different animal (get it? GET IT?)\\n\\nGME Gang lets gooooooo!!!!!!!!\\n\\n28k shares here across 3 accounts\",\n",
       "   \"You think you're the only one that sees that? That isn't the future of Gamestop. A pivot to customer focused e-commerce is what we'll see soon enough.\"],\n",
       "  1610578800: ['Age of selling cd for games is over. Games are downloadable now. Consoles can be bought cheap and sold high on offer up and host of other apps. I sold my ps4 for 300 on offerup. But I know I can buy cheap ps4. \\nGamestop=blockbuster'],\n",
       "  1610751600: []},\n",
       " 'SPY': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610492400: [\"BE THEY CRISPY, OR FROM *WENDY'S*\"],\n",
       "  1610578800: [],\n",
       "  1610751600: []},\n",
       " 'TWTTR': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610751600: []},\n",
       " 'Twitter': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610492400: ['Inorganic brand engagement: An autistic way\\n\\nI see, people creating memes and post to encourage herd buying of GME shares. In fact, I joined in myself. \\n\\nI was thinking if people could boost brand engagements of a company in a similar way. The ticker is GOEV. What if 10-20% people on this sub decided to follow and engage with Canoo Incâ€™s social media accounts/posts e.g. following their page, liking and commenting their posts. This could provide a 5-10 fold increase in their engagement rate. Better brand engagement would mean better awareness of the company and their product. A retarded way to think would be less advertising expense which means more money for R&D. MoreðŸ’° in R&D- better product- higher stock prices.\\n\\nIf this is not deleted my mods, go and follow Canoo Inc. in Instagram, Twitter, LinkedIn etc. Like and comment. GOEV to the moon ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       "   '>Twitter account is a Fortnite character\\n\\nðŸ˜',\n",
       "   'Spam his Twitter',\n",
       "   'I thought it was hacked? \\nRemember when Twitter accounts got hacked? That was during that time'],\n",
       "  1610578800: [],\n",
       "  1610751600: [\"DFV is Roaring Kitty on YouTube and Twitter.. Just putting it out there because I don't think the rules let him promote that .. and the dude is a fucking hero\"]},\n",
       " 'TSLA': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610492400: ['This might break the market in the morning boys. Im in with a boat load of money already. I prolly wonâ€™t be able to sleep. The question now is do we keep for another 2 months? AAPL vs TSLA war might have just started',\n",
       "   'GME fanlevel is almost on TSLA level rn',\n",
       "   'Bro top comment was â€œTSLA $3500 Sept Call option highest availableâ€\\nThat would have printed',\n",
       "   'how much would he have printed on the top comment\\n\\n>[SantiTrades](https://www.reddit.com/user/SantiTrades/)19.5k pointsÂ·[6 months ago](https://www.reddit.com/r/wallstreetbets/comments/hrorfa/i_will_invest_100000_into_whatever_is_the_top/fy5f4hr/?utm_source=reddit&utm_medium=web2x&context=3)Â·*edited 6 months ago*ðŸ“·3ðŸ“·ðŸ“·ðŸ“·& 29 More  \\n>  \\n>TSLA $3500 Sept Call option highest available',\n",
       "   'He is likely going to be first person who started their career on YouTube to reach billionaire net worth. If he would have bought TSLA calls, he could have been first.... ummm... whatâ€™s after a billion?',\n",
       "   'So TSLA 1k by 1/15 confirmed?',\n",
       "   'I was beginning to worry that the LOTR meme creators had turned against us and abandoned us for $TSLA; however, I should have known their return was imminent as they have returned with their tendies in support! I should not have doubted the autism within the sub that is my only regret.',\n",
       "   'Elon being Elon. So genuine. â€œHey you... Yeah you Queen... Youâ€™re gonna make it! ðŸ’•ðŸ’•â€ Earning calls 4Q by 27 Jan. Easy money as it will hit $1.000 easily. Good luck for everyone believe on TSLA',\n",
       "   'TSLA=PLTR\\nRUN=RUN\\nPress X',\n",
       "   'You shorted TSLA huh? Chin up your like Michael Burry, just with a fraction of his money but all of his autism'],\n",
       "  1610578800: ['A new age of humanity? Finally a smart person in charge? This guy will take us to the red planet? Probably...\\n\\nBUT FIRST, HIJACKING YOUR POST:\\n\\nBUY GME ðŸš€ðŸš€ðŸš€ (and TSLA for sure for Elon, BUT FIRST GME ðŸš€ðŸš€ðŸš€)'],\n",
       "  1610751600: ['Been in a while....keep on it.  I\\'m very long personally.  What other \"space\" company can you invest in these days since Orbital went bust that is only space and not diversified?  I know TSLA moves to SpaceX but still....',\n",
       "   'You literally gave your money to the market makers on a silver platter. S1000 strike for next weeks expiry? TSLA has one of the highest theta decay rates',\n",
       "   \"Dumb fuck. We're doing GME this week, not TSLA\",\n",
       "   'Tesla hit me pretty hard this week too. Not going to let it stop me though. Just need more time. TSLA for lifeðŸš€']},\n",
       " 'Tesla': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610492400: [\"I don't what to believe... Some say Apple is working with Hyundai, then others say Tesla is begging them to go to bed together. In the beginning it was Magna, Now here come GOEV?\\n\\nIn Taiwan, everyone is crazy about Hong Hai (Foxxconn), apparently the iphone make is making iphone with 4 wheels too!\\n\\nThink I better start another one with retards here and founding members... EV that can meme and fly!\",\n",
       "   'Exactly, it was $4 and itâ€™s GameStop. Nothing innovative or new, atleast Tesla and Apple have tangible products and a devout following.\\n\\nNobody has a good experience there, thereâ€™s no A+ in customer support, and they are a dying breed of a company, they are basically RedBox.',\n",
       "   \"Please research PPC advertising and other digital marketing channels (in which Cohen has extensive experience in), and then get back to me. That's just one of many ways to substantially grow GME's revenue and profits. You don't need to invent the wheel to be a very successful business. I'm in digital marketing and made $400K in 2019. I don't do anything special like Tesla.\",\n",
       "   'Regular lurker here but I didnâ€™t realize till now in Mr. beastâ€™s post someone comments, â€œIâ€™m gayâ€ and another person replies, â€œyou heard the man, short Tesla.â€ Im crying laughing.',\n",
       "   'I actually followed this call. Sep 2020 calls got IV crushed after the S&P \"snubbed\" Tesla and then expired worthless. Jan 2021 calls woulda worked tho ðŸš€ðŸš€ðŸš€',\n",
       "   'We overestimate wsbâ€™s power. It can move the needle somewhat but letâ€™s not pretend that the big firms are not bandwagoning or whatever it is they do to cash in on the Trend. Do people seriously believe that wsb or retail for that matter added 500 billion to Teslaâ€™s market cap.',\n",
       "   'Tesla has doubled since I bought in. Whose the bitch now!',\n",
       "   \"My friend just got hired at Tesla in R&D. He told me that part of the hiring process is final approval from Papa Musk. So I'm essentially one person removed from the Man.\",\n",
       "   'Cheer up buddy, they only have paper gains....\\n\\n\\n.... just kidding, I am sitting on a stash of $1mil of Tesla from my original $100k investment last year.',\n",
       "   \"I try not to think about what my shares from 2017 would be worth now. I knew everything about Musk and Tesla and was one of those obnoxious people who wouldn't shut up about it. But I was new to investing and convinced that it was overvalued and becoming a bubble and so on...\"],\n",
       "  1610578800: ['no wonder Tesla is overweight'],\n",
       "  1610751600: ['There are so many great partnerships that John Chen their CEO has secured. They already are partnered with 61% of the EV market. There are many other reasons but this is one of the big ones. Do some research on BB you will see that their transition from a phone company to a tech/security company is akin to Tesla from a car company to a tech company.',\n",
       "   'Honestly the Hussein thing is very split in the long community. Half day itâ€™s bad half day itâ€™s good. Honestly I donâ€™t think it effected stock price really at all. From what it appears they hat sold some phone patents to Huawei which doesnâ€™t really matter as BB is no longer a phone company.\\n\\nAmazon said they are 50/50 with BB in exclusive deal. Where I believe Amazon will produce the physical device that goes in the cars to track everything and deploy the cyber security.\\n\\nThey are partnered with 61% of the EV manufacturers currently and are working on more. Not sure if that was before or after Sony partnership tho. As well for fun in one of the last meetings they said Tesla could absolutely be a partner in the near future.\\n\\nTheir security software has so much potential as it can be used in pretty much everything especially with the future ahead with things like autonomous driving, smart cities, robo taxi networks. The list of things that both need unbreakable security and the ability to track many metrics and use them for enhancements and training of AI.\\n\\nThe potential is insane.',\n",
       "   \"And we didn't even have a successful test flight and Richard flying to the moon with a naked dick and ducking the shit out of it. \\n\\nWe will be soaring past Tesla by then.\",\n",
       "   'Tesla hit me pretty hard this week too. Not going to let it stop me though. Just need more time. TSLA for lifeðŸš€',\n",
       "   'are you me ? got those same Tesla calls today lol we got this. See you on the ðŸš€ next week',\n",
       "   \"If you were here since last week, it should be gme, bb or pltr. Tesla wasn't talked about for two weeks already\",\n",
       "   'Stop buying Tesla Jesus Christ all the gains are from people who bought this shit years or months ago \\n\\nTheta alone will bleed all you options and you stupid fucks deserve it']},\n",
       " 'AMD': {1610233200: [],\n",
       "  1610319600: [],\n",
       "  1610492400: [],\n",
       "  1610578800: [],\n",
       "  1610751600: [\"I did the exact opposite..\\n\\nYOLO'd everything into purchase 2680 shares of GME  @41.34  during pre market.  My paper hands sold it at almost the effing bottom at $34.20..   $17850 evaporated lost all my gains since November.  \\n\\n\\nI Then took the remains and purchased  AMD. and some calls for VALE 18 Jun 20C\\n\\n\\nCame home from work and  found out I really f'd up and should have not sold.... repurchase shares in my long term account at ETrade and my webull account...I was able to scrape together 1/3 of what I had this morning\\n\\n.....I foolishly paper handed a lambo  buyt may be able to get a rusted out Honda with this combo...\"]}}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8688/2035823247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcomment_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'comment_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3453\u001b[0m         ...           compression=compression_opts)  # doctest: +SKIP\n\u001b[0;32m   3454\u001b[0m         \"\"\"\n\u001b[1;32m-> 3455\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m         formatter = DataFrameFormatter(\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_frame'"
     ]
    }
   ],
   "source": [
    "comment_data = pd.DataFrame.from_dict(ticker_dict)\n",
    "pd.DataFrame.to_csv('comment_data.csv', comment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Comments Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame()\n",
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "# all_comments = get_comments_by_date(start, end)\n",
    "ticker_dates_dict = {}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    ticker_dates_dict[tick] = pd.DataFrame()\n",
    "\n",
    "# Dates begin yesterday through today:\n",
    "end_day = datetime.today() # End day is today\n",
    "start_day = end_day - timedelta(days=7) # Start day is one week ag\n",
    "\n",
    "# Running for the past week:\n",
    "for tick in tickerlist:\n",
    "    for i in range(0, 7):\n",
    "        end_day = datetime.today() - timedelta(days=i)\n",
    "        start_day = datetime.today() - timedelta(days=i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dates_dict['SPY']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
