{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Reddit Comment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"p1dG7hgoowK4BSlUdar1WQ\",\n",
    "    client_secret=\"pEePtSnw7KMDZi6fCzkKaOth_pgKpQ\",\n",
    "    password=\"outdoortuesday\",\n",
    "    user_agent=\"Big Data by u/DISWillJayminMaya \",\n",
    "    username=\"DISWillJayminMaya \",\n",
    "    prawcore_timeout = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def returnListofScores(listofcomments):\n",
    "    templist=[]\n",
    "    for com in listofcomments:\n",
    "        temp = analyzer.polarity_scores(com)\n",
    "        compScore = temp['compound']\n",
    "        templist.append(compScore)\n",
    "    return templist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "subreddit = reddit.subreddit('wallstreetbets')\n",
    "\n",
    "scoreDict={key:list() for key in tickerlist}\n",
    "\n",
    "hot = subreddit.hot(limit=25) # getting first 15 posts in the 'hot' section of the subreddit\n",
    "sum = [0] * len(tickerlist) # our output array\n",
    "counttotal = 0 # total number of comment read\n",
    "submissions_counter = 0\n",
    "\n",
    "rel_comments = [] # List of comments that are relevant to the ticker list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returnCompScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16416/4024818855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[0mrel_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                         \u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturnCompScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                         \u001b[0mscoreDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'returnCompScore' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop for fetching the comments and the amount of time each ticker is mentioned\n",
    "for submissions in hot:\n",
    "    if not submissions.stickied:\n",
    "        submissions_counter+=1\n",
    "        if submissions_counter > 5:\n",
    "            comments = submissions.comments\n",
    "            for comment in comments:\n",
    "                if isinstance(comment, MoreComments):\n",
    "                    continue\n",
    "                counttotal+=1\n",
    "                for i, ticker in enumerate(tickerlist):\n",
    "                    if ticker in comment.body:\n",
    "                        rel_comments.append(comment.body)\n",
    "                        sum[i]=sum[i]+1\n",
    "                        temp = returnCompScore(comment.body)\n",
    "                        scoreDict[ticker].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDict={key:list() for key in tickerlist}\n",
    "spreadDict={key:list() for key in tickerlist}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    if len(scoreDict[tick])>0:\n",
    "        meanDict[tick]=statistics.mean(scoreDict[tick])\n",
    "    if len (scoreDict[tick])>1:\n",
    "        spreadDict[tick]=statistics.stdev(scoreDict[tick])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n",
      "{'GME': [], 'Gamestop': [], 'SPY': [], 'TWTTR': [], 'Twitter': [], 'TSLA': [], 'Tesla': [], 'AMD': []}\n"
     ]
    }
   ],
   "source": [
    "print(meanDict)\n",
    "print(spreadDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments read:  249\n",
      "  Tick  Counts\n",
      "0  GME       1\n"
     ]
    }
   ],
   "source": [
    "output=pd.DataFrame(data={'Tick': tickerlist, 'Counts': sum})\n",
    "print('Total comments read: ',counttotal)\n",
    "print(output[output['Counts']>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Stock Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json, csv\n",
    "\n",
    "size = 'compact' # 'full' for complete historical data, 'compact' for most recent 100\n",
    "ticker = ['GME', 'SPY', 'TWTTR', 'TSLA', 'AMD'] # stock tickers to search for\n",
    "datatype = 'csv' # 'json' for JSON output, 'csv' for CSV output\n",
    "\n",
    "for stock in ticker:\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={stock}&outputsize={size}&datatype={datatype}&apikey=QC1C7LRPUTLC597Q'\n",
    "    response = requests.get(url)\n",
    "    #Save CSV to file\n",
    "    with open(f'{stock}.csv', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-29</td>\n",
       "      <td>88.05</td>\n",
       "      <td>91.790</td>\n",
       "      <td>85.3800</td>\n",
       "      <td>85.52</td>\n",
       "      <td>82647701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>86.67</td>\n",
       "      <td>90.580</td>\n",
       "      <td>84.7800</td>\n",
       "      <td>89.64</td>\n",
       "      <td>91495449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>84.25</td>\n",
       "      <td>87.900</td>\n",
       "      <td>84.0200</td>\n",
       "      <td>84.91</td>\n",
       "      <td>83125054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>89.74</td>\n",
       "      <td>90.120</td>\n",
       "      <td>85.0800</td>\n",
       "      <td>85.16</td>\n",
       "      <td>87805574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>89.86</td>\n",
       "      <td>91.370</td>\n",
       "      <td>88.6100</td>\n",
       "      <td>90.69</td>\n",
       "      <td>93481042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>138.25</td>\n",
       "      <td>139.400</td>\n",
       "      <td>133.4150</td>\n",
       "      <td>133.80</td>\n",
       "      <td>42173963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>141.29</td>\n",
       "      <td>141.365</td>\n",
       "      <td>135.8200</td>\n",
       "      <td>138.55</td>\n",
       "      <td>42224275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>145.16</td>\n",
       "      <td>146.690</td>\n",
       "      <td>137.8000</td>\n",
       "      <td>138.10</td>\n",
       "      <td>53019926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>144.96</td>\n",
       "      <td>147.040</td>\n",
       "      <td>142.7000</td>\n",
       "      <td>145.24</td>\n",
       "      <td>40977478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>143.90</td>\n",
       "      <td>145.760</td>\n",
       "      <td>141.0001</td>\n",
       "      <td>144.85</td>\n",
       "      <td>53359432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp    open     high       low   close    volume\n",
       "0   2022-04-29   88.05   91.790   85.3800   85.52  82647701\n",
       "1   2022-04-28   86.67   90.580   84.7800   89.64  91495449\n",
       "2   2022-04-27   84.25   87.900   84.0200   84.91  83125054\n",
       "3   2022-04-26   89.74   90.120   85.0800   85.16  87805574\n",
       "4   2022-04-25   89.86   91.370   88.6100   90.69  93481042\n",
       "..         ...     ...      ...       ...     ...       ...\n",
       "95  2021-12-13  138.25  139.400  133.4150  133.80  42173963\n",
       "96  2021-12-10  141.29  141.365  135.8200  138.55  42224275\n",
       "97  2021-12-09  145.16  146.690  137.8000  138.10  53019926\n",
       "98  2021-12-08  144.96  147.040  142.7000  145.24  40977478\n",
       "99  2021-12-07  143.90  145.760  141.0001  144.85  53359432\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amd_data = pd.read_csv(\"AMD.csv\")\n",
    "amd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get avg, spread, median of sentiment of comments in any given period\n",
    "    compare this with the performance of the stock\n",
    "\n",
    "model:\n",
    "    based on last x days of reddit comments, what is the price going to be?\n",
    "    take data from x days, put it all into one vector, and predict from this\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a function to return all comments that mention a stock based on a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def make_request(uri, max_retries = 5):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)\n",
    "\n",
    "def get_intervals(startDate, endDate, daysInInterval = 1):\n",
    "    \"\"\"\n",
    "    get_intervals goes day by day through the start and end dates, returning that day's POSTIX\n",
    "    \"\"\"\n",
    "    # Converting start and end dates to POSTIX:\n",
    "    startPOSTIX = math.floor(startDate.timestamp())\n",
    "    endPOSTIX = math.floor(endDate.timestamp())\n",
    "    # 86,400 seconds in a day:\n",
    "    period = (86400 * daysInInterval)\n",
    "    end = startPOSTIX + period\n",
    "    \n",
    "    yield(int(startPOSTIX), int(end))\n",
    "    \n",
    "    padding = 1\n",
    "    while end <= endPOSTIX:\n",
    "        startPOSTIX = end + padding\n",
    "        end = (startPOSTIX - padding) + period\n",
    "        yield int(startPOSTIX), int(end)\n",
    "    \n",
    "    \n",
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts(\n",
    "        make_request(URI_TEMPLATE.format\n",
    "                     (subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request( \\\n",
    "                URI_TEMPLATE.format( \\\n",
    "                    subreddit, new_start_at, end_at, bSIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "    return post_collections\n",
    "\n",
    "def get_comments_by_date (startDate, endDate, subreddit='wallstreetbets'):\n",
    "    \"\"\"\n",
    "    Takes a given time interval and scrapes the given subreddit for all of the comments\n",
    "    that relate to the given ticker name, returning them as an array. Basic structure taken \n",
    "    from medium article.\n",
    "    \n",
    "    THIS CURRENTLY WILL NOT WORK IF GIVEN TODAY'S DATE. IT WILL ATTEMPT TO FETCH TOMORROW'S POSTS FOREVER\n",
    "    \"\"\"\n",
    "\n",
    "    posts = []\n",
    "    # This loop gets all of the posts in the given timeframe\n",
    "    for interval in get_intervals(startDate, endDate):\n",
    "        print(\"-- Fetching Posts From: \", datetime.fromtimestamp(interval[0]), \" to \", datetime.fromtimestamp(interval[1]))\n",
    "        pulled_posts = pull_posts_for(subreddit, interval[0], interval[1])\n",
    "        posts.extend(pulled_posts)\n",
    "#         time.sleep(.100) # So as not to over request reddit\n",
    "\n",
    "    # Changing time stamps to be readable\n",
    "    \n",
    "    TIMEOUT_SECS = 1\n",
    "    \n",
    "    reddit_posts = []\n",
    "    reddit_comments = {}\n",
    "    startIndex = \"{}-{}-{}\".format(startDate.year, startDate.month, startDate.day)\n",
    "    reddit_comments[startIndex] = []\n",
    "    \n",
    "    # Going through each unique post and comment and adding them to the relevant arrays\n",
    "    #  WARNING: only looking at first 100 posts of each day\n",
    "    for sub_id in np.unique([post['id'] for post in posts])[:100]:\n",
    "        # Only looking at posts with more than 100 upvotes to speed the process up\n",
    "        if reddit.submission(sub_id).ups > 100:\n",
    "            sub = reddit.submission(id=sub_id)\n",
    "            reddit_posts.append(sub)\n",
    "            sub.comments.replace_more(limit=None)\n",
    "            # Looping through each comment:\n",
    "            temp_com_count = 0\n",
    "            for comment in sub.comments.list()[:100]: \n",
    "                temp_com_count += 1\n",
    "                reddit_comments[startIndex].append(comment.body)\n",
    "                \n",
    "            print(\"---- Fethced {} comments from post {}\".format(temp_com_count, sub_id))\n",
    "#             time.sleep(TIMEOUT_SECS)\n",
    "\n",
    "    return reddit_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 11 - Jan 27 2021\n",
    "date_range = []\n",
    "\n",
    "# Filling array with dates (should be 11, 28 for GME Boom)\n",
    "for i in range(11, 28):\n",
    "    date_range.append(datetime(2021, 1, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Fetching Posts From:  2021-01-10 00:00:00  to  2021-01-11 00:00:00\n",
      "-- Fetching Posts From:  2021-01-11 00:00:01  to  2021-01-12 00:00:00\n",
      "---- Fethced 100 comments from post ku2fx9\n",
      "---- Fethced 100 comments from post ku2hgj\n",
      "---- Fethced 37 comments from post ku2kna\n",
      "---- Fethced 100 comments from post ku2ref\n",
      "---- Fethced 57 comments from post ku2ujw\n",
      "---- Fethced 60 comments from post ku2zrg\n",
      "---- Fethced 66 comments from post ku3dkn\n",
      "---- Fethced 100 comments from post ku3ymq\n",
      "---- Fethced 100 comments from post ku400h\n",
      "-- Fetching Posts From:  2021-01-11 00:00:00  to  2021-01-12 00:00:00\n",
      "-- Fetching Posts From:  2021-01-12 00:00:01  to  2021-01-13 00:00:00\n",
      "---- Fethced 53 comments from post kupcnj\n",
      "---- Fethced 100 comments from post kuq6q7\n",
      "---- Fethced 64 comments from post kuqk68\n",
      "---- Fethced 26 comments from post kuqsep\n",
      "---- Fethced 66 comments from post kuqy2m\n",
      "-- Fetching Posts From:  2021-01-12 00:00:00  to  2021-01-13 00:00:00\n",
      "-- Fetching Posts From:  2021-01-13 00:00:01  to  2021-01-14 00:00:00\n",
      "---- Fethced 100 comments from post kvellp\n",
      "---- Fethced 46 comments from post kvf7r1\n",
      "---- Fethced 48 comments from post kvf9od\n",
      "---- Fethced 100 comments from post kvfd4i\n",
      "---- Fethced 100 comments from post kvfgn2\n"
     ]
    }
   ],
   "source": [
    "all_comments = {}\n",
    "for day in date_range:\n",
    "    end_day = day\n",
    "    start_day = day - timedelta(days=1)\n",
    "    temp_all_coms = get_comments_by_date(start_day, end_day)\n",
    "    all_comments.update(temp_all_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "ticker_dict = {}\n",
    "\n",
    "# Filling ticker_dict with empty dictionaries\n",
    "for tick in tickerlist:\n",
    "    ticker_dict[tick] = {}\n",
    "\n",
    "    # Filling the dictionaries in ticker_dict with empty lists\n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        ticker_dict[tick][key] = []\n",
    "        \n",
    "# Adding the comments to their ticker and date\n",
    "for tick in tickerlist:\n",
    "    for key in all_comments.keys():\n",
    "        for com in all_comments[key]:\n",
    "            if tick in com:\n",
    "                ticker_dict[tick][key].append(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2021-1-10'])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GME': {'2021-1-10': ['Why would anyone do this? Of all the companies, you chose GME?',\n",
       "   'What are we looking at here?  You bought 600+ contracts of GME at the $30 strike?  So you put 60k into a call?  \\n\\nI assume you understood the risks going in and you fell on the losing side of that trade, that’s how the market works.  Better luck next time.',\n",
       "   \"Mr. Beast definitely has the funds to take a nice stake in GME. Wonder if he's a gambling man? Oh wait, he just bought a **million** dollars worth of scratch off tickets. Someone tweet the man the DD.\",\n",
       "   'next video better be \"I BUY 10,000 GME shares!!!\"',\n",
       "   'Bullish GME to tha 🌝',\n",
       "   \"I wonder if he could get a Robinhood paid promotion and used all the money to buy GME triggering the moass. It's a win/win/win\",\n",
       "   \"Eh, this was probably sponsored by GME, they either gave him the cash or didn't actually charge him\"]},\n",
       " 'Gamestop': {'2021-1-10': [\"You have some hope next week after Gamestops ICR presentation next Monday 4:30 EST. Maybe Tuesday the stock sees a jump. Cohen tweets seem bullish in a trolly way. But yeah you're prob fucked LMAO. Might as well hold until then.\"]},\n",
       " 'SPY': {'2021-1-10': ['[https://diamondhands.trade/?symbol=SPY&strike=380&exp=2021-03-19&bear=false&sc1=bid\\\\_1545&sc2=ask\\\\_1545](https://diamondhands.trade/?symbol=SPY&strike=380&exp=2021-03-19&bear=false&sc1=bid_1545&sc2=ask_1545)\\n\\nit definitely has march exp data. Regarding those tickers, maybe the contracts only became available yesterday?',\n",
       "   'You have $4 million. Get out of your positions, start selling OTM weekly puts on blue chip stocks or SPY. Easy 6 figure salary for doing nothing. You can retire, travel, live your life, and never have to sit in front of stock charts again. Hell, you could live just off the dividends with what you have. Sell, but not because any of these will drop. Just because you don’t need any more money',\n",
       "   'By $SPY puts, premiums are better, and $TSLA will drag it down, but you have others that can do that as well. I have 20 1/13 378p.']},\n",
       " 'TWTTR': {'2021-1-10': []},\n",
       " 'Twitter': {'2021-1-10': []},\n",
       " 'TSLA': {'2021-1-10': [\"I won't lie, posts like these sends endorphines from the prefrontal cortex straight into my veins after all of the TSLA gains posts from its ~25% run-up this week.\",\n",
       "   'This is awesome, but can you provide me this date BEFORE it has happened... that’ll be great\\n\\nAnd that TSLA call... fuck me!!!!',\n",
       "   'That TSLA 120x bagger',\n",
       "   'Rule number fucking 1 and 2 bruh. Never bet against Elon. Never short TSLA. Cover and hold man.',\n",
       "   \"Rekt. I never held TSLA consistently; held it through various ETFS indirectly. Couldn't validate the cost but now diving deeper into the solar and home battery etc ive actually put in market orders for Monday 10 mins ago.\\n\\nYou're rekt close those shorts this stock is not a car company, its not a technology company... its a fucking utility company sucking juice from the sun to power all the stuff it sells.\",\n",
       "   'He shorted it, meaning he took a bearish position on TSLA like an absolute tard',\n",
       "   \"And you held that position since then.. as in continued to balloon the fuck up.\\n\\nWell if you're hoping TSLA will drop back to below 330....  you might be MORE insane than the people that think It'll cross 1000.\",\n",
       "   \"I'd love to see a sizeable dip so I can get on that train.\\n\\nTSLA is cult status in retail but now the institutions have bought in with S&P add.\\n\\nI don't think a nice drop is coming any time soon.\\n\\nGod speed short bus passenger!\",\n",
       "   'Fuck me dead. Your worst performing position is up 100%... and it’s TSLA shares... what the fuck',\n",
       "   \"I'm gonna buy 50k in TSLA calls, just to make it crash and spite you.\",\n",
       "   \">Never selling\\n\\nI don't understand this, you could pull $3 million out and literally be set for life just living off dividend trading rather than autistic options trading\\n\\nInstead, if the TSLA bubble pops ur fucked\",\n",
       "   'I will personally bet you a “$TSLA” tattoo that it hits 900 next week.',\n",
       "   \"I'm convinced that the only way for TSLA to come down is if the rest of the markets comes with it.\",\n",
       "   \"TSLA\\u3000doesn't not go down. I have been waiting for a long time now..\",\n",
       "   'TSLA = Future energy resources, possible consolidation of all assets including satellites in fucking space \\n\\nBubble = Boomers giving anyone with a pulse a mortgage + packaging any mortgages into securities \\n\\nTesla will be a $10,000 stock before I retire (age 33)',\n",
       "   \"Last year the longest bullish streak for a stock was 12 days. Friday was TSLA's 12th day. Let's see if they can break the record.\",\n",
       "   'By $SPY puts, premiums are better, and $TSLA will drag it down, but you have others that can do that as well. I have 20 1/13 378p.',\n",
       "   \"The actual day when TSLA goes down is when the biggest funds who bought TSLA held for an entire year so that they wouldn't have to pay as much tax begin to sell. Then you have dip buyers fomo supporting the rest of the funds to completely exit their positions. I expect March or later.\",\n",
       "   'TSLA is making people rich, the American dream coming true, why is your post such a bear?!?!.',\n",
       "   \"TSLA's stock price is completely unsustainable but you'll need balls of steel to short them.\",\n",
       "   'Come on man we all know TSLA only goes up',\n",
       "   'TSLA made huge run last few weeks without any major unexpected positive news. S&P 500 + minor milestones.\\n\\nBut now we have 160k touch screen recall, big fail in Europe (btw biggest EV market) and bad quality assembly pushing 500k.\\n\\nTogather with covid realted bad news across the board i feel we will have a bigger pullback sooner than expected.',\n",
       "   'Agreed. I own puts right now and plan to open more next week. $TSLA <$200 by next year is entirely plausible (and arguably likely).',\n",
       "   \"Shorts lost 38 billion last year. 1bilion in one day on jan. 1. You don't want to make money??\\nGo with the trend make some money. Old money bought and locked 20-30% of the float In the past month. Its safer than it ever been. Tsla bubble bears amaze me. TSLA TO MARS\\n🚀 🚀🚀🚀🚀🚀🚀🚀🚀\",\n",
       "   'can you do it on your butt checks?\\n\\nTS left side & LA right side? Just so I can bow to you when I see you. TSLA 1500 1/15',\n",
       "   '/u/PsychiatricPatrick\\n\\nYesterday and Today, I bought TSLA shares and calls nicely at bottom ! Added more around 635-640 range !',\n",
       "   \"you're seeing it wrong. The only way for the market to come down is if TSLA comes down\"]},\n",
       " 'Tesla': {'2021-1-10': ['No movement on Monday? Sell and put them into Tesla',\n",
       "   'Tesla needs help?',\n",
       "   \"If you're going to just give your money to Elon you might as well buy a Tesla\",\n",
       "   'I thought only Mr. Pleas Fly Again was capable of losing money on Tesla during the recent run.  You need a catch phrase for us to remember you by.  Pleas Charge Again?',\n",
       "   \"There's still chance of 20- 30% drop with institution correction base on this spike as they adjust portfolio size. It's just when and you'll have to quickly get out because I suspect lots of people going to go back in.\\n\\nIf you hold out for too long and Tesla decided to do another round of stock split... then it's going to be scary.\\n\\nbest of luck.\",\n",
       "   \"Crazy how people with insane gains on Tesla don't want to sell cause ' tax' . All of these are unrealized gains and the stock could crash tomorrow making all the gains seem a lot lesser. The stock has had a parabolic run and is due for correction. Pay the frikking tax man and live a peaceful life ffs..\",\n",
       "   'When Tesla split... did you have to close all these out and buy back in?',\n",
       "   'Sell half at least , imagine waking up on a random Monday morning and someone had declared war on the u.s. and Dow futures is down 2000 and Tesla is sitting at $400....... that $4 mill ain’t shit unless it’s in your bank account ..... congrats and fuck you as well',\n",
       "   'IMO Tesla will drop only after starlink goes public, maybe because of starlink going public.',\n",
       "   'TSLA = Future energy resources, possible consolidation of all assets including satellites in fucking space \\n\\nBubble = Boomers giving anyone with a pulse a mortgage + packaging any mortgages into securities \\n\\nTesla will be a $10,000 stock before I retire (age 33)',\n",
       "   'I think your right. I dont know what \"soon\" is but I cashed out my Tesla gains Friday night. 2k to 10kish was good enough for me and I dont wanna be there for the coming price correction.',\n",
       "   \"You guys better watch out. I'm finally joining in on the Tesla train watch it go down next week 🙃🚀🚀🚀\",\n",
       "   'Bla bla bla. Tesla is going down. Is that anyone posts anymore?',\n",
       "   'Next week when. Ps. If it doesn’t hit $900 and you don’t want a tattoo you can buy me one share of Tesla.',\n",
       "   'Tesla going down baby,  get ready for  downward ride',\n",
       "   'This is what scares me. I’m not in Tesla, but a bunch of others in the sector. I feel like Tesla is the benchmark for all of them.']},\n",
       " 'AMD': {'2021-1-10': []}}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data = pd.DataFrame.from_dict(ticker_dict)\n",
    "comment_data.to_csv('comment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GME</th>\n",
       "      <th>Gamestop</th>\n",
       "      <th>SPY</th>\n",
       "      <th>TWTTR</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>Tesla</th>\n",
       "      <th>AMD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1610233200</th>\n",
       "      <td>[Why would anyone do this? Of all the companie...</td>\n",
       "      <td>[You have some hope next week after Gamestops ...</td>\n",
       "      <td>[[https://diamondhands.trade/?symbol=SPY&amp;strik...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I won't lie, posts like these sends endorphin...</td>\n",
       "      <td>[No movement on Monday? Sell and put them into...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          GME  \\\n",
       "1610233200  [Why would anyone do this? Of all the companie...   \n",
       "\n",
       "                                                     Gamestop  \\\n",
       "1610233200  [You have some hope next week after Gamestops ...   \n",
       "\n",
       "                                                          SPY TWTTR Twitter  \\\n",
       "1610233200  [[https://diamondhands.trade/?symbol=SPY&strik...    []      []   \n",
       "\n",
       "                                                         TSLA  \\\n",
       "1610233200  [I won't lie, posts like these sends endorphin...   \n",
       "\n",
       "                                                        Tesla AMD  \n",
       "1610233200  [No movement on Monday? Sell and put them into...  []  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Comments Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame()\n",
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "# all_comments = get_comments_by_date(start, end)\n",
    "ticker_dates_dict = {}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    ticker_dates_dict[tick] = pd.DataFrame()\n",
    "\n",
    "# Dates begin yesterday through today:\n",
    "end_day = datetime.today() # End day is today\n",
    "start_day = end_day - timedelta(days=7) # Start day is one week ag\n",
    "\n",
    "# Running for the past week:\n",
    "for tick in tickerlist:\n",
    "    for i in range(0, 7):\n",
    "        end_day = datetime.today() - timedelta(days=i)\n",
    "        start_day = datetime.today() - timedelta(days=i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dates_dict['SPY']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
