{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Reddit Comment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"p1dG7hgoowK4BSlUdar1WQ\",\n",
    "    client_secret=\"pEePtSnw7KMDZi6fCzkKaOth_pgKpQ\",\n",
    "    password=\"outdoortuesday\",\n",
    "    user_agent=\"Big Data by u/DISWillJayminMaya \",\n",
    "    username=\"DISWillJayminMaya \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "subreddit = reddit.subreddit('wallstreetbets')\n",
    "\n",
    "hot = subreddit.hot(limit=25) # getting first 15 posts in the 'hot' section of the subreddit\n",
    "sum = [0] * len(tickerlist) # our output array\n",
    "counttotal = 0 # total number of comment read\n",
    "submissions_counter = 0\n",
    "\n",
    "rel_comments = [] # List of comments that are relevant to the ticker list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for fetching the comments and the amount of time each ticker is mentioned\n",
    "for submissions in hot:\n",
    "    if not submissions.stickied:\n",
    "        submissions_counter+=1\n",
    "        if submissions_counter > 5:\n",
    "            comments = submissions.comments\n",
    "            for comment in comments:\n",
    "                if isinstance(comment, MoreComments):\n",
    "                    continue\n",
    "                counttotal+=1\n",
    "                for i, ticker in enumerate(tickerlist):\n",
    "                    if ticker in comment.body:\n",
    "                        rel_comments.append(comment.body)\n",
    "                        sum[i]=sum[i]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments read:  1193\n",
      "       Tick  Counts\n",
      "0       GME       8\n",
      "1  Gamestop       1\n",
      "2       SPY      10\n",
      "4   Twitter      13\n",
      "5      TSLA      23\n",
      "6     Tesla      28\n"
     ]
    }
   ],
   "source": [
    "output=pd.DataFrame(data={'Tick': tickerlist, 'Counts': sum})\n",
    "print('Total comments read: ',counttotal)\n",
    "print(output[output['Counts']>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Stock Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json, csv\n",
    "\n",
    "size = 'compact' # 'full' for complete historical data, 'compact' for most recent 100\n",
    "ticker = ['GME', 'SPY', 'TWTTR', 'TSLA', 'AMD'] # stock tickers to search for\n",
    "datatype = 'csv' # 'json' for JSON output, 'csv' for CSV output\n",
    "\n",
    "for stock in ticker:\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={stock}&outputsize={size}&datatype={datatype}&apikey=QC1C7LRPUTLC597Q'\n",
    "    response = requests.get(url)\n",
    "    #Save CSV to file\n",
    "    with open(f'{stock}.csv', 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>89.740</td>\n",
       "      <td>90.12</td>\n",
       "      <td>85.0800</td>\n",
       "      <td>85.16</td>\n",
       "      <td>87805574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>89.860</td>\n",
       "      <td>91.37</td>\n",
       "      <td>88.6100</td>\n",
       "      <td>90.69</td>\n",
       "      <td>93481042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>90.030</td>\n",
       "      <td>91.46</td>\n",
       "      <td>87.9350</td>\n",
       "      <td>88.14</td>\n",
       "      <td>75017652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>95.160</td>\n",
       "      <td>96.23</td>\n",
       "      <td>89.2400</td>\n",
       "      <td>89.85</td>\n",
       "      <td>76680635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-20</td>\n",
       "      <td>97.840</td>\n",
       "      <td>97.91</td>\n",
       "      <td>93.2000</td>\n",
       "      <td>94.02</td>\n",
       "      <td>62322274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>144.960</td>\n",
       "      <td>147.04</td>\n",
       "      <td>142.7000</td>\n",
       "      <td>145.24</td>\n",
       "      <td>40977478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>143.900</td>\n",
       "      <td>145.76</td>\n",
       "      <td>141.0001</td>\n",
       "      <td>144.85</td>\n",
       "      <td>53359432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>141.135</td>\n",
       "      <td>141.31</td>\n",
       "      <td>134.2000</td>\n",
       "      <td>139.06</td>\n",
       "      <td>66776493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>151.645</td>\n",
       "      <td>152.38</td>\n",
       "      <td>140.7200</td>\n",
       "      <td>144.01</td>\n",
       "      <td>65910028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2021-12-02</td>\n",
       "      <td>147.675</td>\n",
       "      <td>152.53</td>\n",
       "      <td>146.4700</td>\n",
       "      <td>150.68</td>\n",
       "      <td>56161216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp     open    high       low   close    volume\n",
       "0   2022-04-26   89.740   90.12   85.0800   85.16  87805574\n",
       "1   2022-04-25   89.860   91.37   88.6100   90.69  93481042\n",
       "2   2022-04-22   90.030   91.46   87.9350   88.14  75017652\n",
       "3   2022-04-21   95.160   96.23   89.2400   89.85  76680635\n",
       "4   2022-04-20   97.840   97.91   93.2000   94.02  62322274\n",
       "..         ...      ...     ...       ...     ...       ...\n",
       "95  2021-12-08  144.960  147.04  142.7000  145.24  40977478\n",
       "96  2021-12-07  143.900  145.76  141.0001  144.85  53359432\n",
       "97  2021-12-06  141.135  141.31  134.2000  139.06  66776493\n",
       "98  2021-12-03  151.645  152.38  140.7200  144.01  65910028\n",
       "99  2021-12-02  147.675  152.53  146.4700  150.68  56161216\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amd_data = pd.read_csv(\"AMD.csv\")\n",
    "amd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get avg, spread, median of sentiment of comments in any given period\n",
    "    compare this with the performance of the stock\n",
    "\n",
    "model:\n",
    "    based on last x days of reddit comments, what is the price going to be?\n",
    "    take data from x days, put it all into one vector, and predict from this\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a function to return all comments that mention a stock based on a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def make_request(uri, max_retries = 5):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)\n",
    "\n",
    "def get_intervals(startPOSTIX, endPOSTIX, daysInInterval = 1):\n",
    "    \"\"\"\n",
    "    get_intervals goes day by day through the start and end dates, returning that day's POSTIX\n",
    "    \"\"\"\n",
    "    # 86,400 seconds in a day:\n",
    "    period = (86400 * daysInInterval)\n",
    "    end = startPOSTIX + period\n",
    "    \n",
    "    yield(int(startPOSTIX), int(end))\n",
    "    \n",
    "    padding = 1\n",
    "    while end <= endPOSTIX:\n",
    "        startPOSTIX = end + padding\n",
    "        end = (startPOSTIX - padding) + period\n",
    "        yield int(startPOSTIX), int(end)\n",
    "    \n",
    "    \n",
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \"\"\"\n",
    "    Function taken from medium article:\n",
    "    https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "    \"\"\"\n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts(\n",
    "        make_request(URI_TEMPLATE.format\n",
    "                     (subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request( \\\n",
    "                URI_TEMPLATE.format( \\\n",
    "                    subreddit, new_start_at, end_at, bSIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "    return post_collections\n",
    "\n",
    "def get_comments_by_date (startDate, endDate, subreddit='wallstreetbets'):\n",
    "    \"\"\"\n",
    "    Takes a given time interval and scrapes the given subreddit for all of the comments\n",
    "    that relate to the given ticker name, returning them as an array. Basic structure taken \n",
    "    from medium article.\n",
    "    \"\"\"\n",
    "    # Converting start and end dates to POSTIX:\n",
    "    startDate = math.floor(startDate.timestamp())\n",
    "    endDate = math.floor(endDate.timestamp())\n",
    "\n",
    "    posts = []\n",
    "    # This loop gets all of the posts in the given timeframe\n",
    "    for interval in get_intervals(startDate, endDate):\n",
    "        print(\" -- Fetching Posts From: \", datetime.fromtimestamp(interval[0]), \" to \", datetime.fromtimestamp(interval[1]))\n",
    "        pulled_posts = pull_posts_for(subreddit, interval[0], interval[1])\n",
    "        posts.extend(pulled_posts)\n",
    "        time.sleep(.100) # So as not to over request reddit\n",
    "    \n",
    "    TIMEOUT_SECS = .100\n",
    "    \n",
    "    reddit_posts = []\n",
    "    reddit_comments = []\n",
    "    # Going through each unique post and comment and adding them to the relevant arrays\n",
    "    for sub_id in np.unique([post['id'] for post in posts]):\n",
    "        sub = reddit.submission(id=sub_id)\n",
    "        reddit_posts.append(sub)\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        # Looping through each comment:\n",
    "        for comment in sub.comments.list():\n",
    "            reddit_comments.append(comment.body)\n",
    "                \n",
    "            time.sleep(TIMEOUT_SECS)\n",
    "    \n",
    "    return reddit_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Fetching Posts From:  2022-04-27 10:09:56  to  2022-04-28 10:09:56\n"
     ]
    }
   ],
   "source": [
    "start = datetime.today() - timedelta(days=0.1)\n",
    "end = datetime.today()\n",
    "\n",
    "tickerlist = ['GME', 'Gamestop', 'SPY', 'TWTTR', 'Twitter', 'TSLA', 'Tesla', 'AMD']\n",
    "\n",
    "all_comments = get_comments_by_date(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Too bearish in old thread. SPY 425 today',\n",
       " 'SPY could drop another $20 today and it still wouldnâ€™t be half as Gay as Mods',\n",
       " 'SPY will dump at open.  We all know it.',\n",
       " 'SPY PUMP IS FAKE',\n",
       " 'Too many bears. SPY to $450',\n",
       " 'SPY 440 by Thursday',\n",
       " 'SPY +1% after falling -3% is textbook dead cat bounce example.',\n",
       " \"do you mean the gap up in SPY? Because of GOOGL and MSFT financial reports. It doesn't necessary mean, that the gap would hold up.\"]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_dict = {}\n",
    "\n",
    "for tick in tickerlist:\n",
    "    ticker_dict[tick] = []\n",
    "    \n",
    "for com in all_comments:\n",
    "    for tick in tickerlist:\n",
    "        if tick in com:\n",
    "            ticker_dict[tick].append(com)\n",
    "            \n",
    "ticker_dict['SPY']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
